#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DGA Detection - ç®€å•è®­ç»ƒè„šæœ¬
"""

import torch
import pickle
import os
import sys
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.append(project_root)

from core.dataset import create_data_loaders, print_dataset_info
import numpy as np
from sklearn.model_selection import train_test_split
from core.base_model import ModelTrainer
from core.improved_trainer import ImprovedModelTrainer
from core.enhanced_trainer import EnhancedModelTrainer
from core.enhanced_moe_trainer import EnhancedMoETrainer
from config.config import config
from models.implementations.cnn_model import CNNModel
from models.implementations.lstm_model import LSTMModel
from models.implementations.mamba_model import MambaModel
from models.implementations.moe_model import MoEModel
from models.implementations.improved_moe_model import ImprovedMoEModel
from models.implementations.simplified_improved_moe_model import SimplifiedImprovedMoEModel
from models.implementations.specialized_experts import (
    CharacterLevelExpert, DictionaryLevelExpert, BiGRUAttentionExpert, 
    CNNWithCBAMExpert, TransformerExpert
)
from models.implementations.specialized_moe_model import SpecializedMoEModel
from models.implementations.enhanced_mambaformer_model import EnhancedMambaFormerModel
from models.implementations.advanced_mambaformer_moe import AdvancedMambaFormerMoE
from models.implementations.homogeneous_mambaformer_moe import HomogeneousMambaFormerMoE
from models.implementations.tcbam_models import TCBAMModel
from models.implementations.homogeneous_tcbam_moe import HomogeneousTCBAMMoE
from models.implementations.enhanced_homogeneous_tcbam_moe import EnhancedHomogeneousTCBAMMoE
from models.implementations.mambaformer_model import MambaFormerModel


def get_model(model_name: str, vocab_size: int, num_classes: int = 2):
    """æ ¹æ®åç§°åˆ›å»ºæ¨¡å‹"""
    models = {
        'cnn': CNNModel,
        'lstm': LSTMModel,
        'mamba': MambaModel,
        'moe': MoEModel,
        'improved_moe': ImprovedMoEModel,
        'simplified_moe': SimplifiedImprovedMoEModel,
        'mambaformer': MambaFormerModel,
        # ä¸“é—¨åŒ–ä¸“å®¶æ¨¡å‹
        'char_expert': CharacterLevelExpert,
        'dict_expert': DictionaryLevelExpert,
        'bigru_att': BiGRUAttentionExpert,
        'cnn_cbam': CNNWithCBAMExpert,
        'transformer_expert': TransformerExpert,
        # ä¸“é—¨åŒ–MoEæ¨¡å‹
        'specialized_moe_char_dict': lambda vocab_size, num_classes: SpecializedMoEModel(vocab_size, expert_config="char_dict", num_classes=num_classes),
        'specialized_moe_advanced': lambda vocab_size, num_classes: SpecializedMoEModel(vocab_size, expert_config="advanced", num_classes=num_classes),
        'specialized_moe_hybrid': lambda vocab_size, num_classes: SpecializedMoEModel(vocab_size, expert_config="hybrid", num_classes=num_classes),
        # å¢å¼ºMambaFormeræ¨¡å‹
        'enhanced_mambaformer': EnhancedMambaFormerModel,
        # TCBAMæ¨¡å‹
        'tcbam': TCBAMModel,
        # åŒæ„TCBAM-MoEæ¨¡å‹
        'homogeneous_tcbam_moe': lambda vocab_size, num_classes: HomogeneousTCBAMMoE(vocab_size, num_classes=num_classes, num_experts=4),
        # å¢å¼ºç‰ˆåŒæ„TCBAM-MoEæ¨¡å‹
        'enhanced_tcbam_moe': lambda vocab_size, num_classes: EnhancedHomogeneousTCBAMMoE(vocab_size, num_classes=num_classes, num_experts=4),
        # é«˜çº§MambaFormer MoEæ¨¡å‹
        'advanced_moe_mambaformer': lambda vocab_size, num_classes: AdvancedMambaFormerMoE(vocab_size, expert_config="mambaformer_only", num_classes=num_classes),
        'advanced_moe_hybrid': lambda vocab_size, num_classes: AdvancedMambaFormerMoE(vocab_size, expert_config="hybrid_advanced", num_classes=num_classes),
        'advanced_moe_ultimate': lambda vocab_size, num_classes: AdvancedMambaFormerMoE(vocab_size, expert_config="advanced", num_classes=num_classes),
        # åŒæ„MambaFormer MoEæ¨¡å‹
        'homogeneous_moe_4experts': lambda vocab_size, num_classes: HomogeneousMambaFormerMoE(vocab_size, num_experts=4, num_classes=num_classes),
        'homogeneous_moe_6experts': lambda vocab_size, num_classes: HomogeneousMambaFormerMoE(vocab_size, num_experts=6, num_classes=num_classes),
        'homogeneous_moe_8experts': lambda vocab_size, num_classes: HomogeneousMambaFormerMoE(vocab_size, num_experts=8, num_classes=num_classes)
    }
    
    if model_name.lower() not in models:
        raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_name}. å¯é€‰: {list(models.keys())}")
    
    model_class = models[model_name.lower()]
    
    # å¤„ç†lambdaå‡½æ•°çš„ç‰¹æ®Šæƒ…å†µ
    if callable(model_class) and not isinstance(model_class, type):
        # å¯¹äºlambdaå‡½æ•°ï¼Œç›´æ¥è°ƒç”¨
        return model_class(vocab_size, num_classes)
    
    if model_name.lower() == 'mambaformer':
        # MambaFormerä½¿ç”¨ç‰¹æ®Šå‚æ•°
        return model_class(
            vocab_size=vocab_size, 
            d_model=config.model.d_model, 
            n_layers=3,
            d_state=16,
            n_heads=4,
            num_classes=num_classes, 
            dropout=config.model.dropout,
            fusion_type='gated'  # é»˜è®¤ä½¿ç”¨é—¨æ§èåˆ
        )
    elif model_name.lower() == 'tcbam':
        # TCBAMä½¿ç”¨ç‰¹æ®Šå‚æ•°
        return model_class(
            vocab_size=vocab_size,
            num_classes=num_classes,
            embed_dim=config.model.d_model,
            hidden_dim=128,
            num_filters=128,
            num_heads=8,
            num_layers=2,
            dropout=config.model.dropout
        )
    else:
        return model_class(
            vocab_size=vocab_size, 
            d_model=config.model.d_model, 
            num_classes=num_classes, 
            dropout=config.model.dropout
        )


def train_model(model_name: str, quick_test: bool = False, dataset_size: str = 'small'):
    """è®­ç»ƒæŒ‡å®šæ¨¡å‹"""
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {model_name.upper()} æ¨¡å‹")
    print("=" * 60)
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # é€‰æ‹©æ•°æ®é›†è·¯å¾„
    dataset_paths = {
        'small': config.data.dataset_path,
        'medium': './data/processed/medium_dga_dataset.pkl',
        'large': './data/processed/large_dga_dataset.pkl',
        'xlarge': './data/processed/xlarge_dga_dataset.pkl',
        'small_multiclass': './data/processed/small_multiclass_dga_dataset.pkl',
        'medium_multiclass': './data/processed/medium_multiclass_dga_dataset.pkl',
        'large_multiclass': './data/processed/large_multiclass_dga_dataset.pkl'
    }
    
    dataset_path = dataset_paths.get(dataset_size, config.data.dataset_path)
    print(f"ğŸ“‚ åŠ è½½{dataset_size}æ•°æ®é›†: {dataset_path}")
    
    try:
        train_loader, val_loader, test_loader, dataset_info = create_data_loaders(
            dataset_path=dataset_path,
            batch_size=config.training.batch_size,
            train_ratio=config.data.train_ratio,
            val_ratio=config.data.val_ratio,
            random_seed=config.training.random_seed
        )
        print_dataset_info(dataset_info)
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        print("è¯·ç¡®ä¿æ•°æ®é›†æ–‡ä»¶å­˜åœ¨ï¼Œæˆ–å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬")
        return None
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸  åˆ›å»º {model_name.upper()} æ¨¡å‹...")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šåˆ†ç±»æ•°æ®é›†
    num_classes = dataset_info.get('num_classes', 2)
    if 'multiclass' in dataset_size:
        print(f"  æ£€æµ‹åˆ°å¤šåˆ†ç±»æ•°æ®é›†ï¼Œç±»åˆ«æ•°: {num_classes}")
    
    model = get_model(model_name, dataset_info['vocab_size'], num_classes)
    model.print_model_info()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    if model_name == 'improved_moe':
        trainer = ImprovedModelTrainer(model, device, load_balance_weight=0.1)
        print(f"  ä½¿ç”¨æ”¹è¿›è®­ç»ƒå™¨ï¼Œè´Ÿè½½å‡è¡¡æƒé‡: 0.1")
    elif model_name == 'enhanced_tcbam_moe':
        trainer = EnhancedMoETrainer(model, device, load_balance_weight=0.01, diversity_weight=0.01)
        print(f"  ä½¿ç”¨å¢å¼ºMoEè®­ç»ƒå™¨ï¼Œè´Ÿè½½å‡è¡¡æƒé‡: 0.01, å¤šæ ·æ€§æƒé‡: 0.01")
    elif (model_name == 'simplified_moe' or model_name.startswith('specialized_moe') or 
          model_name.startswith('advanced_moe') or model_name.startswith('homogeneous_moe')):
        trainer = EnhancedModelTrainer(model, device)
        print(f"  ä½¿ç”¨å¢å¼ºè®­ç»ƒå™¨ï¼Œæ”¯æŒå¤šæ ·æ€§æŸå¤±")
    else:
        trainer = ModelTrainer(model, device)
    
    # è®­ç»ƒå‚æ•° - ä¸ºMoEæ¨¡å‹å¢åŠ æ›´å¤šè®­ç»ƒè½®æ•°
    if 'moe' in model_name.lower():
        epochs = 10 if quick_test else 50  # MoEæ¨¡å‹éœ€è¦æ›´å¤šè®­ç»ƒ
    else:
        epochs = 5 if quick_test else config.training.num_epochs
    
    # å¼€å§‹è®­ç»ƒ
    save_path = config.get_model_save_path(model_name)
    training_results = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=epochs,
        learning_rate=config.training.learning_rate,
        weight_decay=config.training.weight_decay,
        patience=config.training.patience,
        save_path=save_path
    )
    
    # åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¯„ä¼°
    if os.path.exists(save_path):
        model.load_state_dict(torch.load(save_path))
        print(f"\\nğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹...")
        test_results = trainer.evaluate(test_loader)
        
        # ä¸“å®¶ä½¿ç”¨åˆ†æï¼ˆä»…å¯¹MoEæ¨¡å‹ï¼‰
        expert_analysis = None
        if model_name == 'improved_moe' and hasattr(trainer, 'analyze_expert_usage'):
            print(f"\\nğŸ” åˆ†æä¸“å®¶ä½¿ç”¨æƒ…å†µ...")
            expert_analysis = trainer.analyze_expert_usage(test_loader)
        elif (model_name == 'simplified_moe' or model_name.startswith('specialized_moe') or 
              model_name.startswith('advanced_moe') or model_name.startswith('homogeneous_moe')) and hasattr(trainer, 'analyze_expert_usage_detailed'):
            print(f"\\nğŸ” è¯¦ç»†åˆ†æä¸“å®¶ä½¿ç”¨æƒ…å†µ...")
            expert_analysis = trainer.analyze_expert_usage_detailed(test_loader)
        
        # ä¿å­˜ç»“æœ
        results = {
            'model_name': model_name,
            'model_info': model.get_model_info(),
            'training_results': training_results,
            'test_results': test_results,
            'expert_analysis': expert_analysis
        }
        
        results_path = config.get_results_save_path(model_name)
        with open(results_path, 'wb') as f:
            pickle.dump(results, f)
        
        print(f"\\nâœ… {model_name.upper()} è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {training_results['best_val_accuracy']:.2f}%")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_results['accuracy']*100:.2f}%")
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        
        return results
    else:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {save_path}")
        return None


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='DGAæ£€æµ‹æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--model', type=str, default='cnn', 
                       choices=[
                             'cnn', 'lstm', 'mamba', 'moe', 'improved_moe', 'simplified_moe', 'mambaformer',
                             'char_expert', 'dict_expert', 'bigru_att', 'cnn_cbam', 'transformer_expert',
                             'specialized_moe_char_dict', 'specialized_moe_advanced', 'specialized_moe_hybrid',
                             'enhanced_mambaformer', 'tcbam', 'homogeneous_tcbam_moe', 'enhanced_tcbam_moe',
                             'advanced_moe_mambaformer', 'advanced_moe_hybrid', 'advanced_moe_ultimate',
                              'homogeneous_moe_4experts', 'homogeneous_moe_6experts', 'homogeneous_moe_8experts'
                         ],
                       help='è¦è®­ç»ƒçš„æ¨¡å‹ç±»å‹')
    parser.add_argument('--quick', action='store_true', 
                       help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆ5ä¸ªepochï¼‰')
    parser.add_argument('--all', action='store_true', 
                       help='è®­ç»ƒæ‰€æœ‰æ¨¡å‹')
    parser.add_argument('--dataset', type=str, default='small', 
                        choices=['small', 'medium', 'large', 'xlarge', 'small_multiclass', 'medium_multiclass', 'large_multiclass'],
                        help='æ•°æ®é›†å¤§å°')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(config.training.random_seed)
    
    if args.all:
        print("ğŸ¯ è®­ç»ƒæ‰€æœ‰æ¨¡å‹...")
        models = ['cnn', 'lstm', 'mamba', 'moe', 'mambaformer']
        results = {}
        
        for model_name in models:
            print(f"\\n{'='*80}")
            result = train_model(model_name, args.quick, args.dataset)
            if result:
                results[model_name] = result
            print(f"{'='*80}")
        
        print(f"\\nğŸ† æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"è®­ç»ƒäº† {len(results)} ä¸ªæ¨¡å‹")
        
        # ç®€å•å¯¹æ¯”
        if results:
            print(f"\\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
            print(f"{'æ¨¡å‹':<10} {'éªŒè¯å‡†ç¡®ç‡':<12} {'æµ‹è¯•å‡†ç¡®ç‡':<12} {'å‚æ•°é‡':<12}")
            print("-" * 50)
            for name, result in results.items():
                val_acc = result['training_results']['best_val_accuracy']
                test_acc = result['test_results']['accuracy'] * 100
                params = result['model_info']['total_params']
                print(f"{name.upper():<10} {val_acc:<11.2f}% {test_acc:<11.2f}% {params:<11,}")
    else:
        train_model(args.model, args.quick, args.dataset)


if __name__ == "__main__":
    main()
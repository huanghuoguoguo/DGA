# DGA恶意域名检测实验结果记录

## 实验设置
- **数据集**: large_multiclass (19,000样本，9个DGA家族)
- **训练设置**: 快速训练模式 (5个epoch)
- **设备**: CPU
- **数据分布**: 训练集13,300，验证集1,900，测试集3,800

## 实验结果对比

### 模型性能对比表

| 模型 | 测试准确率(%) | 验证准确率(%) | 参数量(M) | 模型大小(MB) | 推理时间(ms) | 训练时间(s) |
|------|---------------|---------------|-----------|-------------|-------------|-------------|
| **同构TCBAM-MoE** | **68.24** | 67.26 | 5.97 | 22.76 | 83.45 | 1066.53 |
| TCBAM | 68.55 | 70.05 | 1.45 | 5.53 | 20.76 | 282.30 |
| LSTM | **70.32** | **71.26** | 0.70 | 2.67 | 6.20 | 62.81 |
| CNN | 61.50 | 61.42 | 0.20 | 0.78 | 2.14 | 22.52 |

### 详细性能指标

#### 同构TCBAM-MoE
- 测试准确率: 68.24%
- 精确率: 67.03%
- 召回率: 68.24%
- F1分数: 64.51%
- 专家数量: 4个
- 专家类型: TCBAM (同构)

#### TCBAM (单一模型)
- 测试准确率: 68.55%
- 精确率: 62.85%
- 召回率: 68.55%
- F1分数: 64.89%

#### LSTM
- 测试准确率: 70.32%
- 精确率: 70.68%
- 召回率: 70.32%
- F1分数: 68.23%

#### CNN
- 测试准确率: 61.50%
- 精确率: 67.86%
- 召回率: 61.50%
- F1分数: 56.98%

## 实验分析

### 1. 性能分析

**意外发现**：
- LSTM模型表现最佳，测试准确率达到70.32%
- 单一TCBAM模型(68.55%)略优于同构TCBAM-MoE(68.24%)
- MoE架构并未带来预期的性能提升

**可能原因**：
1. **数据规模限制**: 19,000样本对于MoE模型可能不够充分
2. **专家分工不明确**: 同构专家可能学习到相似的特征模式
3. **门控网络简单**: 当前的线性门控可能无法有效分配专家权重
4. **训练不充分**: 仅5个epoch可能不足以让MoE收敛到最优

### 2. 效率分析

**参数效率**：
- CNN最轻量: 0.20M参数，0.78MB
- LSTM平衡性好: 0.70M参数，推理时间6.20ms
- TCBAM-MoE参数量最大: 5.97M参数，但性能提升有限

**推理效率**：
- CNN最快: 2.14ms
- LSTM次之: 6.20ms  
- TCBAM-MoE最慢: 83.45ms (4倍专家网络开销)

### 3. 训练效率

**训练时间对比**：
- CNN: 22.52s (最快)
- LSTM: 62.81s
- TCBAM: 282.30s
- TCBAM-MoE: 1066.53s (最慢，约18分钟)

## 问题诊断与优化建议

### 1. MoE架构问题

**当前问题**：
- 同构专家缺乏差异化
- 门控网络过于简单
- 负载均衡可能不佳

**优化方向**：
1. **异构专家设计**: 一个专家处理字符级特征，另一个处理字典级特征
2. **增强门控网络**: 使用多层MLP替代线性门控
3. **特征工程**: 明确区分不同类型的输入特征
4. **负载均衡损失**: 添加专家使用均衡的正则化项

### 2. 数据集分析

**数据特点**：
- 9个DGA家族，类别相对均衡
- 可能存在家族间特征重叠
- 需要更细粒度的特征分析

**改进建议**：
1. 分析不同DGA家族的特征分布
2. 设计针对性的特征提取策略
3. 考虑增加数据集规模

### 3. 基线模型优势

**LSTM表现最佳的原因**：
1. 序列建模能力强，适合域名字符序列
2. 参数量适中，避免过拟合
3. 训练效率高，容易收敛

**启示**：
- 简单有效的模型在中等规模数据上可能更优
- MoE架构需要更大规模数据才能发挥优势
- 需要重新评估MoE的必要性

## 增强实验结果（增加训练轮数后）

### 增强版实验对比

| 模型 | 训练轮数 | 测试准确率(%) | 验证准确率(%) | 参数量(M) | 推理时间(ms) | 训练时间(s) |
|------|----------|---------------|---------------|-----------|-------------|-------------|
| **增强TCBAM-MoE** | 10 | **65.26** | 65.58 | 6.42 | 64.82 | 1689.18 |
| **原版TCBAM-MoE** | 10 | **68.89** | 68.53 | 5.97 | 80.63 | 2158.98 |
| 原版TCBAM-MoE | 5 | 68.24 | 67.26 | 5.97 | 83.45 | 1066.53 |
| TCBAM (单一) | 5 | 68.55 | 70.05 | 1.45 | 20.76 | 282.30 |
| LSTM | 5 | **70.32** | **71.26** | 0.70 | 6.20 | 62.81 |

### 重要发现

#### 1. **增加训练轮数的效果**
- **原版TCBAM-MoE**: 从5轮的68.24%提升到10轮的68.89%（+0.65%）
- **增强版TCBAM-MoE**: 10轮达到65.26%，**低于原版**
- **训练轮数确实有帮助**，但提升有限

#### 2. **增强版模型的问题**
- **性能下降**: 增强版反而比原版低3.63%
- **参数增加**: 从5.97M增加到6.42M（+7.5%）
- **推理效率提升**: 从80.63ms降到64.82ms（+19.6%）
- **训练更快**: 从2158.98s降到1689.18s（+21.8%）

#### 3. **专家使用分析**
**原版TCBAM-MoE专家使用情况**：
- 专家使用相对均衡，但仍有偏向
- 没有明显的专家特化

**增强版TCBAM-MoE专家使用情况**：
- Epoch 5: ['21.6%', '13.0%', '47.2%', '18.2%'] - 专家2占主导
- Epoch 10: ['18.5%', '10.3%', '57.6%', '13.5%'] - 专家2更加占主导
- **负载不均衡问题严重**，专家2使用率过高（57.6%）

### 问题诊断

#### 1. **增强版设计问题**
- **过度复杂化**: 专家特异性设计可能导致某些专家过于特化
- **负载均衡失效**: 尽管有负载均衡损失，但专家使用仍然不均衡
- **专家冲突**: 不同的初始化策略可能导致专家间性能差异过大

#### 2. **原版的优势**
- **简单有效**: 同构设计虽然简单，但更稳定
- **训练稳定**: 专家间差异较小，训练更容易收敛
- **负载相对均衡**: 虽然不完美，但比增强版好

#### 3. **LSTM仍然最优的原因**
- **架构适配**: 序列建模天然适合域名字符序列
- **参数效率**: 0.70M参数是最优配置
- **训练简单**: 单一架构，无复杂的专家协调问题
- **收敛快速**: 62.81秒训练时间，效率极高

### 优化建议

#### 1. **立即可行的改进**
1. **简化增强版设计**: 减少专家间的差异化
2. **增强负载均衡**: 提高负载均衡损失权重
3. **统一初始化**: 使用相同的初始化策略
4. **渐进式训练**: 先训练单个专家，再联合优化

#### 2. **架构重新设计**
1. **回归简单设计**: 保持同构专家的简单性
2. **专注特征工程**: 在输入层区分特征类型，而非专家层
3. **门控网络优化**: 改进门控机制，而非专家架构

#### 3. **实验策略调整**
1. **更多训练轮数**: 尝试20-50轮训练
2. **不同学习率**: 为MoE使用更小的学习率
3. **数据增强**: 扩大训练数据规模

## 结论更新

基于增强实验的结果：

1. **LSTM仍然是最佳选择**：70.32%准确率 + 极高效率
2. **简单的同构MoE优于复杂设计**：原版TCBAM-MoE > 增强版
3. **增加训练轮数有帮助但有限**：提升不到1%
4. **过度工程化是有害的**：复杂的专家设计反而降低性能
5. **负载均衡仍然是关键问题**：需要更好的解决方案

**最重要的启示**：在中等规模数据上，**简单有效的模型往往优于复杂的架构**。MoE的优势需要在更大规模数据和更合适的任务上才能体现。
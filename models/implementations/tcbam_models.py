#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TCBAMæ¨¡å‹å®ç° - æ”¹é€ ç‰ˆ
Transformer + CNN + BiLSTM + Attention + CBAMçš„ç»„åˆæ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from core.base_model import BaseModel


class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æ¨¡å—"""
    
    def __init__(self, in_channels, reduction=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.max_pool = nn.AdaptiveMaxPool1d(1)
        self.fc = nn.Sequential(
            nn.Conv1d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(),
            nn.Conv1d(in_channels // reduction, in_channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        out = avg_out + max_out
        return self.sigmoid(out)


class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æ¨¡å—"""
    
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv1 = nn.Conv1d(2, 1, kernel_size, padding=(kernel_size - 1) // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)


class CBAMBlock(nn.Module):
    """CBAMæ³¨æ„åŠ›å—"""
    
    def __init__(self, in_channels, reduction=16, kernel_size=7):
        super(CBAMBlock, self).__init__()
        self.channel_attention = ChannelAttention(in_channels, reduction)
        self.spatial_attention = SpatialAttention(kernel_size)

    def forward(self, x):
        x = x * self.channel_attention(x)
        x = x * self.spatial_attention(x)
        return x


class DPCNNBlock(nn.Module):
    """æ·±åº¦é‡‘å­—å¡”CNNå—"""
    
    def __init__(self, in_channels, num_filters, kernel_size):
        super(DPCNNBlock, self).__init__()
        self.conv1 = nn.Conv1d(in_channels, num_filters, kernel_size=kernel_size, padding=kernel_size//2)
        self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding=kernel_size//2)
        self.pool = nn.MaxPool1d(2, stride=2)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        # é™åˆ¶æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé¿å…æ— é™å¾ªç¯
        max_iterations = 5
        iteration = 0
        
        while x.size(2) > 2 and iteration < max_iterations:
            # ä¸‹é‡‡æ ·å±‚
            p1 = self.pool(x)
            # ç¬¬ä¸€ä¸ªå·ç§¯å±‚
            c1 = self.activation(self.conv1(p1))
            c1 = self.dropout(c1)
            # ç¬¬äºŒä¸ªå·ç§¯å±‚
            c2 = self.activation(self.conv2(c1))
            c2 = self.dropout(c2)

            # è®¡ç®—è¾ƒå°çš„é•¿åº¦
            min_length = min(c2.size(2), p1.size(2))
            if min_length <= 0:
                break

            # æˆªå–è¾ƒå°é•¿åº¦çš„éƒ¨åˆ†
            p1 = p1[:, :, :min_length]
            c2 = c2[:, :, :min_length]

            # æ®‹å·®è¿æ¥
            x = c2 + p1
            iteration += 1
            
        return x


class BiLSTMAttention(nn.Module):
    """åŒå‘LSTM + æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(BiLSTMAttention, self).__init__()
        self.hidden_dim = hidden_dim
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=0.1)
        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=8, batch_first=True, dropout=0.1)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        lstm_out, _ = self.lstm(x)
        
        # è‡ªæ³¨æ„åŠ›æœºåˆ¶
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        pooled = torch.mean(attn_out, dim=1)
        
        # è¾“å‡ºå±‚
        output = self.fc(self.dropout(pooled))
        return output


class TCBAMModel(BaseModel):
    """TCBAMæ¨¡å‹ï¼šTransformer + CNN + BiLSTM + Attention + CBAM"""
    
    def __init__(self, vocab_size, num_classes=2, embed_dim=128, hidden_dim=128, 
                 num_filters=128, num_heads=8, num_layers=2, dropout=0.1):
        super(TCBAMModel, self).__init__(vocab_size, num_classes)
        
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.num_filters = num_filters
        
        # åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.pos_encoding = nn.Parameter(torch.randn(1000, embed_dim))
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, 
            nhead=num_heads, 
            dim_feedforward=embed_dim * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # CNNåˆ†æ”¯1 (kernel_size=2)
        self.conv1_k2 = nn.Conv1d(embed_dim, num_filters, kernel_size=2, padding=1)
        self.conv2_k2 = nn.Conv1d(num_filters, num_filters, kernel_size=2, padding=1)
        self.cbam1 = CBAMBlock(num_filters)
        self.dpcnn1 = DPCNNBlock(num_filters, num_filters, 2)
        self.bilstm_att1 = BiLSTMAttention(num_filters, hidden_dim//2, num_filters)
        
        # CNNåˆ†æ”¯2 (kernel_size=3)
        self.conv1_k3 = nn.Conv1d(embed_dim, num_filters, kernel_size=3, padding=1)
        self.conv2_k3 = nn.Conv1d(num_filters, num_filters, kernel_size=3, padding=1)
        self.conv3_k3 = nn.Conv1d(num_filters, num_filters, kernel_size=3, padding=1)
        self.cbam2 = CBAMBlock(num_filters)
        self.dpcnn2 = DPCNNBlock(num_filters, num_filters, 3)
        self.bilstm_att2 = BiLSTMAttention(num_filters, hidden_dim//2, num_filters)
        
        # ç‰¹å¾èåˆ
        # DPCNNè¾“å‡ºéœ€è¦å±•å¹³ï¼ŒBiLSTMè¾“å‡ºæ˜¯å›ºå®šç»´åº¦
        self.feature_fusion = nn.Sequential(
            nn.Linear(num_filters * 4, num_filters * 2),  # 4ä¸ªç‰¹å¾æºï¼š2ä¸ªDPCNN + 2ä¸ªBiLSTM
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(num_filters * 2, num_filters),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(num_filters, num_filters // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(num_filters // 2, num_classes)
        )
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        batch_size, seq_len = x.shape
        
        # åµŒå…¥ + ä½ç½®ç¼–ç 
        embedded = self.embedding(x)
        if seq_len <= self.pos_encoding.size(0):
            embedded = embedded + self.pos_encoding[:seq_len].unsqueeze(0)
        
        # Transformerç¼–ç 
        # åˆ›å»ºpadding mask
        padding_mask = (x == 0)
        transformer_out = self.transformer_encoder(embedded, src_key_padding_mask=padding_mask)
        
        # è½¬æ¢ä¸ºCNNè¾“å…¥æ ¼å¼ (batch, channels, seq_len)
        cnn_input = transformer_out.transpose(1, 2)
        
        # åˆ†æ”¯1: kernel_size=2
        c11 = F.relu(self.conv1_k2(cnn_input))
        c11 = self.dropout(c11)
        
        # CBAMæ³¨æ„åŠ›
        cbam1_out = self.cbam1(c11)
        
        c12 = F.relu(self.conv2_k2(c11))
        c12 = self.dropout(c12)
        
        # æ®‹å·®è¿æ¥ + DPCNN (ç¡®ä¿ç»´åº¦åŒ¹é…)
        min_len = min(cbam1_out.size(2), c12.size(2))
        cbam1_out = cbam1_out[:, :, :min_len]
        c12 = c12[:, :, :min_len]
        residual1 = cbam1_out + c12
        dpcnn1_out = self.dpcnn1(residual1)
        
        # BiLSTM + Attention
        bilstm1_input = c12.transpose(1, 2)  # (batch, seq_len, channels)
        bilstm1_out = self.bilstm_att1(bilstm1_input)
        
        # åˆ†æ”¯2: kernel_size=3
        c21 = F.relu(self.conv1_k3(cnn_input))
        c21 = self.dropout(c21)
        
        c22 = F.relu(self.conv2_k3(c21))
        c22 = self.dropout(c22)
        
        c23 = F.relu(self.conv3_k3(c22))
        c23 = self.dropout(c23)
        
        # CBAMæ³¨æ„åŠ›
        cbam2_out = self.cbam2(c23)
        
        # DPCNN
        dpcnn2_out = self.dpcnn2(cbam2_out)
        
        # BiLSTM + Attention
        bilstm2_input = c22.transpose(1, 2)  # (batch, seq_len, channels)
        bilstm2_out = self.bilstm_att2(bilstm2_input)
        
        # ç‰¹å¾èåˆ
        # DPCNNè¾“å‡ºéœ€è¦å…¨å±€å¹³å‡æ± åŒ–
        dpcnn1_pooled = F.adaptive_avg_pool1d(dpcnn1_out, 1).squeeze(-1)
        dpcnn2_pooled = F.adaptive_avg_pool1d(dpcnn2_out, 1).squeeze(-1)
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        combined_features = torch.cat([
            dpcnn1_pooled,  # (batch, num_filters)
            bilstm1_out,    # (batch, num_filters)
            bilstm2_out,    # (batch, num_filters)
            dpcnn2_pooled   # (batch, num_filters)
        ], dim=1)
        
        # ç‰¹å¾èåˆ
        fused_features = self.feature_fusion(combined_features)
        
        # åˆ†ç±»
        logits = self.classifier(fused_features)
        
        return logits
    
    def print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        model_size_mb = total_params * 4 / (1024 * 1024)  # å‡è®¾float32
        
        print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        print(f"  æ€»å‚æ•°é‡: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB")
        print(f"  è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        print(f"  ç±»åˆ«æ•°: {self.num_classes}")

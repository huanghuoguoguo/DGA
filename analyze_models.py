#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DGA Detection - ç»Ÿä¸€æ¨¡å‹åˆ†æå’Œå¯¹æ¯”è„šæœ¬
"""

import torch
import pickle
import os
import sys
import argparse
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.append(project_root)

try:
    from core.dataset import create_data_loaders, print_dataset_info
    from core.base_model import ModelTrainer
    from config.config import config
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿é¡¹ç›®ç»“æ„å®Œæ•´")


def load_all_results():
    """åŠ è½½æ‰€æœ‰æ¨¡å‹çš„å®éªŒç»“æœ"""
    results = {}
    results_dir = config.paths['results_dir']
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»“æœæ–‡ä»¶
    if os.path.exists(results_dir):
        for file in os.listdir(results_dir):
            if file.endswith('_results.pkl'):
                model_name = file.replace('_results.pkl', '').replace('_', ' ').title()
                try:
                    with open(os.path.join(results_dir, file), 'rb') as f:
                        results[model_name] = pickle.load(f)
                    print(f"âœ… åŠ è½½ç»“æœ: {model_name}")
                except Exception as e:
                    print(f"âŒ åŠ è½½å¤±è´¥ {file}: {e}")
    
    # å°è¯•åŠ è½½legacyç»“æœæ–‡ä»¶
    legacy_files = [
        ('./data/training_results.pkl', 'CNN'),
        ('./data/final_test_results.pkl', 'Final Test'),
    ]
    
    for file_path, model_type in legacy_files:
        if os.path.exists(file_path):
            try:
                with open(file_path, 'rb') as f:
                    data = pickle.load(f)
                    if model_type == 'Final Test' and isinstance(data, dict):
                        # è§£æfinal_test_results.pklä¸­çš„å¤šä¸ªæ¨¡å‹
                        for name, result in data.items():
                            if isinstance(result, dict) and 'accuracy' in result:
                                results[name] = {
                                    'test_results': result,
                                    'model_info': {'total_params': result.get('params', 0)}
                                }
                    else:
                        results[model_type] = data
                print(f"âœ… åŠ è½½legacyç»“æœ: {model_type}")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {file_path}: {e}")
    
    return results


def print_comparison_table(results):
    """æ‰“å°æ¨¡å‹å¯¹æ¯”è¡¨æ ¼"""
    if not results:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹ç»“æœ")
        return
    
    print("\n" + "="*100)
    print("ğŸ” DGAæ¶æ„åŸŸåæ£€æµ‹ - æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("="*100)
    
    # è¡¨å¤´
    print(f"{'æ¨¡å‹':<25} {'å‡†ç¡®ç‡':<10} {'F1åˆ†æ•°':<10} {'æ¨ç†æ—¶é—´(ms)':<12} {'å‚æ•°é‡':<12} {'å¤§å°(MB)':<10}")
    print("-" * 100)
    
    # æ•´ç†æ•°æ®
    model_data = []
    for model_name, data in results.items():
        try:
            # æå–æµ‹è¯•ç»“æœ
            if 'test_results' in data:
                test_data = data['test_results']
                accuracy = test_data.get('accuracy', 0)
                f1_score = test_data.get('f1', 0)
                inference_time = test_data.get('inference_time_ms', 0)
            else:
                # å…¼å®¹æ—§æ ¼å¼
                accuracy = data.get('test_accuracy', data.get('accuracy', 0))
                f1_score = data.get('test_f1', data.get('f1', 0))
                inference_time = data.get('avg_inference_time_ms', data.get('inference_time_ms', 0))
            
            # æå–æ¨¡å‹ä¿¡æ¯
            if 'model_info' in data:
                model_info = data['model_info']
                params = model_info.get('total_params', 0)
            else:
                params = data.get('model_params', data.get('params', 0))
            
            # æ ¼å¼åŒ–æ•°æ®
            if accuracy < 1:
                accuracy *= 100
            
            size_mb = params * 4 / 1024 / 1024 if params > 0 else 0
            
            model_data.append({
                'name': model_name,
                'accuracy': accuracy,
                'f1': f1_score,
                'inference_time': inference_time,
                'params': params,
                'size_mb': size_mb
            })
            
        except Exception as e:
            print(f"âš ï¸  è§£ææ¨¡å‹æ•°æ®å¤±è´¥ {model_name}: {e}")
            continue
    
    # æŒ‰å‡†ç¡®ç‡æ’åº
    model_data.sort(key=lambda x: x['accuracy'], reverse=True)
    
    # æ‰“å°æ•°æ®
    for data in model_data:
        print(f"{data['name']:<25} {data['accuracy']:<9.2f}% {data['f1']:<9.4f} "
              f"{data['inference_time']:<11.2f} {data['params']:<11,} {data['size_mb']:<9.2f}")
    
    print("="*100)
    
    # æ€§èƒ½æ€»ç»“
    if model_data:
        best_acc = max(model_data, key=lambda x: x['accuracy'])
        best_f1 = max(model_data, key=lambda x: x['f1'])
        fastest = min(model_data, key=lambda x: x['inference_time'] if x['inference_time'] > 0 else float('inf'))
        smallest = min(model_data, key=lambda x: x['params'] if x['params'] > 0 else float('inf'))
        
        print(f"\nğŸ† æ€§èƒ½æ€»ç»“:")
        print(f"  æœ€é«˜å‡†ç¡®ç‡: {best_acc['name']} ({best_acc['accuracy']:.2f}%)")
        print(f"  æœ€ä½³F1åˆ†æ•°: {best_f1['name']} ({best_f1['f1']:.4f})")
        print(f"  æœ€å¿«æ¨ç†: {fastest['name']} ({fastest['inference_time']:.2f}ms)")
        print(f"  æœ€å°æ¨¡å‹: {smallest['name']} ({smallest['params']:,}å‚æ•°)")
    
    return model_data


def create_comparison_chart(model_data, output_path='./data/results/model_comparison.png'):
    """åˆ›å»ºæ¨¡å‹å¯¹æ¯”å›¾è¡¨"""
    if not model_data:
        return
    
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    names = [d['name'] for d in model_data]
    accuracies = [d['accuracy'] for d in model_data]
    f1_scores = [d['f1'] for d in model_data]
    inference_times = [d['inference_time'] for d in model_data if d['inference_time'] > 0]
    inf_names = [d['name'] for d in model_data if d['inference_time'] > 0]
    params = [d['params']/1000 for d in model_data if d['params'] > 0]  # è½¬æ¢ä¸ºK
    param_names = [d['name'] for d in model_data if d['params'] > 0]
    
    # å‡†ç¡®ç‡å¯¹æ¯”
    bars1 = ax1.bar(names, accuracies, color='skyblue', alpha=0.7)
    ax1.set_title('Model Accuracy Comparison')
    ax1.set_ylabel('Accuracy (%)')
    ax1.tick_params(axis='x', rotation=45)
    for i, v in enumerate(accuracies):
        ax1.text(i, v + 0.5, f'{v:.1f}%', ha='center', va='bottom')
    
    # F1åˆ†æ•°å¯¹æ¯”
    bars2 = ax2.bar(names, f1_scores, color='lightgreen', alpha=0.7)
    ax2.set_title('Model F1-Score Comparison')
    ax2.set_ylabel('F1-Score')
    ax2.tick_params(axis='x', rotation=45)
    for i, v in enumerate(f1_scores):
        ax2.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
    
    # æ¨ç†æ—¶é—´å¯¹æ¯”
    if inference_times:
        bars3 = ax3.bar(inf_names, inference_times, color='salmon', alpha=0.7)
        ax3.set_title('Model Inference Time Comparison')
        ax3.set_ylabel('Inference Time (ms)')
        ax3.tick_params(axis='x', rotation=45)
        for i, v in enumerate(inference_times):
            ax3.text(i, v + max(inference_times)*0.02, f'{v:.1f}', ha='center', va='bottom')
    
    # å‚æ•°é‡å¯¹æ¯”
    if params:
        bars4 = ax4.bar(param_names, params, color='gold', alpha=0.7)
        ax4.set_title('Model Parameters Comparison')
        ax4.set_ylabel('Parameters (K)')
        ax4.tick_params(axis='x', rotation=45)
        for i, v in enumerate(params):
            ax4.text(i, v + max(params)*0.02, f'{v:.0f}K', ha='center', va='bottom')
    
    plt.tight_layout()
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"ğŸ“Š å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")


def analyze_expert_usage(results):
    """åˆ†æMoEæ¨¡å‹çš„ä¸“å®¶ä½¿ç”¨æƒ…å†µ"""
    print(f"\nğŸ”§ MoEæ¨¡å‹ä¸“å®¶ä½¿ç”¨åˆ†æ:")
    
    for model_name, data in results.items():
        if 'moe' in model_name.lower() or 'expert' in model_name.lower():
            if 'test_results' in data and 'expert_usage' in data['test_results']:
                usage = data['test_results']['expert_usage']
                print(f"  {model_name}:")
                if isinstance(usage, dict):
                    for expert, ratio in usage.items():
                        print(f"    {expert}: {ratio:.1%}")
                elif isinstance(usage, (list, tuple)):
                    expert_names = ['CNN', 'LSTM', 'Transformer', 'Mamba']
                    for i, ratio in enumerate(usage):
                        if i < len(expert_names):
                            print(f"    {expert_names[i]}: {ratio:.1%}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='DGAæ£€æµ‹æ¨¡å‹åˆ†æå’Œå¯¹æ¯”')
    parser.add_argument('--chart', action='store_true', help='ç”Ÿæˆå¯¹æ¯”å›¾è¡¨')
    parser.add_argument('--expert', action='store_true', help='åˆ†æä¸“å®¶ä½¿ç”¨æƒ…å†µ')
    parser.add_argument('--output', type=str, default='./data/results/analysis_output.png',
                       help='å›¾è¡¨è¾“å‡ºè·¯å¾„')
    
    args = parser.parse_args()
    
    print("ğŸš€ DGAæ£€æµ‹æ¨¡å‹åˆ†æå·¥å…·")
    print("=" * 50)
    
    # åŠ è½½æ‰€æœ‰ç»“æœ
    print("ğŸ“‚ åŠ è½½æ¨¡å‹ç»“æœ...")
    results = load_all_results()
    
    if not results:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹ç»“æœæ–‡ä»¶")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬: python simple_train.py --all")
        return
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    model_data = print_comparison_table(results)
    
    # åˆ†æä¸“å®¶ä½¿ç”¨æƒ…å†µ
    if args.expert:
        analyze_expert_usage(results)
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    if args.chart and model_data:
        create_comparison_chart(model_data, args.output)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼å…±åˆ†æäº† {len(results)} ä¸ªæ¨¡å‹")


if __name__ == "__main__":
    main()
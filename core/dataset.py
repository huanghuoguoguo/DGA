#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DGA Detection - ç»Ÿä¸€æ•°æ®é›†å¤„ç†æ¨¡å—
"""

import torch
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import pickle
import os
from typing import Tuple, Dict, Any


class DGADataset(Dataset):
    """ç»Ÿä¸€çš„DGAæ•°æ®é›†ç±»"""
    
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.long)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


def load_dataset(file_path: str = './data/processed/small_dga_dataset.pkl') -> Dict[str, Any]:
    """åŠ è½½é¢„å¤„ç†çš„æ•°æ®é›†"""
    if not os.path.exists(file_path):
        # å°è¯•åŸå§‹ä½ç½®
        fallback_path = './data/small_dga_dataset.pkl'
        if os.path.exists(fallback_path):
            file_path = fallback_path
        else:
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    with open(file_path, 'rb') as f:
        dataset = pickle.load(f)
    return dataset


def create_data_loaders(dataset_path: str = './data/processed/small_dga_dataset.pkl',
                       batch_size: int = 32,
                       train_ratio: float = 0.7,
                       val_ratio: float = 0.15,
                       random_seed: int = 42) -> Tuple[DataLoader, DataLoader, DataLoader, Dict[str, Any]]:
    """åˆ›å»ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset(dataset_path)
    
    # æ£€æŸ¥æ•°æ®é›†ç»“æ„ç±»å‹
    if 'train' in dataset and 'val' in dataset and 'test' in dataset:
        # æ–°æ ¼å¼çš„xlarge_multiclassæ•°æ®é›†
        train_data = dataset['train']
        val_data = dataset['val']
        test_data = dataset['test']
        info = dataset['info']
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        X_train = np.array(train_data['sequences'])
        y_train = np.array(train_data['labels'])
        X_val = np.array(val_data['sequences'])
        y_val = np.array(val_data['labels'])
        X_test = np.array(test_data['sequences'])
        y_test = np.array(test_data['labels'])
        
        # åˆ›å»ºæ•°æ®é›†å¯¹è±¡
        train_dataset = DGADataset(X_train, y_train)
        val_dataset = DGADataset(X_val, y_val)
        test_dataset = DGADataset(X_test, y_test)
        
        # æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'vocab_size': info['vocab_size'],
            'max_length': info.get('max_length', X_train.shape[1]),
            'num_classes': info['num_classes'],
            'class_names': [f'class_{i}' for i in range(info['num_classes'])],
            'total_samples': info['total_samples'],
            'train_samples': len(train_dataset),
            'val_samples': len(val_dataset),
            'test_samples': len(test_dataset),
            'class_distribution': np.bincount(y_train)
        }
        
    elif 'X_train' in dataset:
        # æ—§æ ¼å¼çš„å¤šåˆ†ç±»æ•°æ®é›†
        X_train, y_train = dataset['X_train'], dataset['y_train']
        X_val, y_val = dataset['X_val'], dataset['y_val']
        X_test, y_test = dataset['X_test'], dataset['y_test']
        
        # åˆ›å»ºæ•°æ®é›†å¯¹è±¡
        train_dataset = DGADataset(X_train, y_train)
        val_dataset = DGADataset(X_val, y_val)
        test_dataset = DGADataset(X_test, y_test)
        
        # æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'vocab_size': dataset['vocab_size'],
            'max_length': dataset.get('max_length', X_train.shape[1]),
            'num_classes': dataset.get('num_classes', 2),
            'class_names': dataset.get('class_names', ['benign', 'malicious']),
            'total_samples': len(X_train) + len(X_val) + len(X_test),
            'train_samples': len(train_dataset),
            'val_samples': len(val_dataset),
            'test_samples': len(test_dataset),
            'class_distribution': np.bincount(y_train)
        }
        
    else:
        # äºŒåˆ†ç±»æ•°æ®é›†éœ€è¦åˆ†å‰²
        X, y = dataset['X'], dataset['y']
        
        # åˆ›å»ºæ•°æ®é›†å¯¹è±¡
        full_dataset = DGADataset(X, y)
        
        # è®¡ç®—åˆ’åˆ†å¤§å°
        total_size = len(full_dataset)
        train_size = int(train_ratio * total_size)
        val_size = int(val_ratio * total_size)
        test_size = total_size - train_size - val_size
        
        # æ•°æ®é›†åˆ’åˆ†
        train_dataset, val_dataset, test_dataset = random_split(
            full_dataset, [train_size, val_size, test_size],
            generator=torch.Generator().manual_seed(random_seed)
        )
        
        # æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'vocab_size': dataset['vocab_size'],
            'max_length': dataset.get('max_length', X.shape[1]),
            'num_classes': 2,
            'class_names': ['benign', 'malicious'],
            'total_samples': total_size,
            'train_samples': len(train_dataset),
            'val_samples': len(val_dataset),
            'test_samples': len(test_dataset),
            'class_distribution': np.bincount(y)
        }
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ä¼˜åŒ–GPUåˆ©ç”¨ç‡
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=4,  # å¤šè¿›ç¨‹æ•°æ®åŠ è½½
        pin_memory=True,  # å†…å­˜å›ºå®šï¼ŒåŠ é€ŸGPUä¼ è¾“
        persistent_workers=True  # ä¿æŒå·¥ä½œè¿›ç¨‹æ´»è·ƒ
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=2,
        pin_memory=True,
        persistent_workers=True
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=2,
        pin_memory=True,
        persistent_workers=True
    )
    
    return train_loader, val_loader, test_loader, dataset_info


def print_dataset_info(dataset_info: Dict[str, Any]):
    """æ‰“å°æ•°æ®é›†ä¿¡æ¯"""
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"  æ€»æ ·æœ¬æ•°: {dataset_info['total_samples']}")
    print(f"  è¯æ±‡è¡¨å¤§å°: {dataset_info['vocab_size']}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {dataset_info['max_length']}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {dataset_info['class_distribution']}")
    print(f"  è®­ç»ƒé›†: {dataset_info['train_samples']}")
    print(f"  éªŒè¯é›†: {dataset_info['val_samples']}")
    print(f"  æµ‹è¯•é›†: {dataset_info['test_samples']}")
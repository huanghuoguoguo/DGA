#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DGA Detection - ç»Ÿä¸€æ•°æ®é›†å¤„ç†æ¨¡å—
"""

import torch
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import pickle
import os
from typing import Tuple, Dict, Any


class DGADataset(Dataset):
    """ç»Ÿä¸€çš„DGAæ•°æ®é›†ç±»"""
    
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.long)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


def load_dataset(file_path: str = './data/processed/small_dga_dataset.pkl') -> Dict[str, Any]:
    """åŠ è½½é¢„å¤„ç†çš„æ•°æ®é›†"""
    if not os.path.exists(file_path):
        # å°è¯•åŸå§‹ä½ç½®
        fallback_path = './data/small_dga_dataset.pkl'
        if os.path.exists(fallback_path):
            file_path = fallback_path
        else:
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    with open(file_path, 'rb') as f:
        dataset = pickle.load(f)
    return dataset


def create_data_loaders(dataset_path: str = './data/processed/small_dga_dataset.pkl',
                       batch_size: int = 32,
                       train_ratio: float = 0.7,
                       val_ratio: float = 0.15,
                       random_seed: int = 42) -> Tuple[DataLoader, DataLoader, DataLoader, Dict[str, Any]]:
    """åˆ›å»ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset(dataset_path)
    X, y = dataset['X'], dataset['y']
    
    # åˆ›å»ºæ•°æ®é›†å¯¹è±¡
    full_dataset = DGADataset(X, y)
    
    # è®¡ç®—åˆ’åˆ†å¤§å°
    total_size = len(full_dataset)
    train_size = int(train_ratio * total_size)
    val_size = int(val_ratio * total_size)
    test_size = total_size - train_size - val_size
    
    # æ•°æ®é›†åˆ’åˆ†
    train_dataset, val_dataset, test_dataset = random_split(
        full_dataset, [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(random_seed)
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # æ•°æ®é›†ä¿¡æ¯
    dataset_info = {
        'vocab_size': dataset['vocab_size'],
        'max_length': dataset.get('max_length', X.shape[1]),
        'total_samples': total_size,
        'train_samples': len(train_dataset),
        'val_samples': len(val_dataset),
        'test_samples': len(test_dataset),
        'class_distribution': np.bincount(y)
    }
    
    return train_loader, val_loader, test_loader, dataset_info


def print_dataset_info(dataset_info: Dict[str, Any]):
    """æ‰“å°æ•°æ®é›†ä¿¡æ¯"""
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"  æ€»æ ·æœ¬æ•°: {dataset_info['total_samples']}")
    print(f"  è¯æ±‡è¡¨å¤§å°: {dataset_info['vocab_size']}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {dataset_info['max_length']}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {dataset_info['class_distribution']}")
    print(f"  è®­ç»ƒé›†: {dataset_info['train_samples']}")
    print(f"  éªŒè¯é›†: {dataset_info['val_samples']}")
    print(f"  æµ‹è¯•é›†: {dataset_info['test_samples']}")
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æ•°æ®é›†æ„å»ºå™¨
ç”Ÿæˆæ ¼å¼ç»Ÿä¸€çš„äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»DGAæ•°æ®é›†
æ”¯æŒå°å‹(1ä¸‡)å’Œå¤§å‹(10ä¸‡)æ•°æ®é›†
"""

import pandas as pd
import numpy as np
import pickle
import os
from sklearn.model_selection import train_test_split
from collections import Counter
import random
import string
from typing import Dict, List, Tuple, Any


class UnifiedDGADatasetBuilder:
    """ç»Ÿä¸€DGAæ•°æ®é›†æ„å»ºå™¨"""
    
    def __init__(self):
        # å¸¸è§TLD
        self.tlds = ['.com', '.net', '.org', '.info', '.biz', '.co', '.io', '.me', '.tv', '.cc']
        
        # DGAå®¶æ—é…ç½® - 15ä¸ªä¸»è¦DGAå®¶æ—
        self.dga_families = {
            'conficker': {'weight': 0.12, 'min_len': 8, 'max_len': 15, 'pattern': 'random'},
            'cryptolocker': {'weight': 0.10, 'min_len': 12, 'max_len': 20, 'pattern': 'mixed'},
            'zeus': {'weight': 0.09, 'min_len': 10, 'max_len': 16, 'pattern': 'dictionary'},
            'necurs': {'weight': 0.08, 'min_len': 8, 'max_len': 12, 'pattern': 'random'},
            'matsnu': {'weight': 0.07, 'min_len': 6, 'max_len': 10, 'pattern': 'dictionary'},
            'suppobox': {'weight': 0.07, 'min_len': 8, 'max_len': 14, 'pattern': 'mixed'},
            'tinba': {'weight': 0.06, 'min_len': 7, 'max_len': 12, 'pattern': 'random'},
            'ramdo': {'weight': 0.06, 'min_len': 9, 'max_len': 15, 'pattern': 'mixed'},
            'banjori': {'weight': 0.06, 'min_len': 8, 'max_len': 12, 'pattern': 'random'},
            'corebot': {'weight': 0.05, 'min_len': 10, 'max_len': 18, 'pattern': 'dictionary'},
            'dircrypt': {'weight': 0.05, 'min_len': 12, 'max_len': 20, 'pattern': 'mixed'},
            'kraken': {'weight': 0.05, 'min_len': 8, 'max_len': 14, 'pattern': 'random'},
            'locky': {'weight': 0.05, 'min_len': 16, 'max_len': 25, 'pattern': 'mixed'},
            'pykspa': {'weight': 0.04, 'min_len': 6, 'max_len': 10, 'pattern': 'dictionary'},
            'qakbot': {'weight': 0.05, 'min_len': 8, 'max_len': 16, 'pattern': 'random'}
        }
        
        # å­—å…¸å•è¯ï¼ˆç”¨äºç”Ÿæˆå­—å…¸å‹DGAï¼‰
        self.dictionary_words = [
            'secure', 'bank', 'login', 'account', 'service', 'update', 'verify', 'confirm',
            'support', 'help', 'mail', 'web', 'site', 'page', 'home', 'user', 'admin',
            'system', 'network', 'server', 'client', 'data', 'info', 'news', 'blog',
            'shop', 'store', 'buy', 'sell', 'pay', 'money', 'cash', 'card', 'credit',
            'online', 'digital', 'tech', 'soft', 'app', 'mobile', 'cloud', 'smart',
            'fast', 'quick', 'easy', 'simple', 'best', 'top', 'new', 'free', 'safe'
        ]
        
        # è‰¯æ€§åŸŸåæ¨¡å¼
        self.benign_patterns = [
            'company', 'business', 'service', 'tech', 'digital', 'solutions', 'systems',
            'global', 'international', 'world', 'group', 'corp', 'inc', 'ltd', 'llc'
        ]
    
    def generate_random_string(self, min_len: int, max_len: int, charset: str = None) -> str:
        """ç”Ÿæˆéšæœºå­—ç¬¦ä¸²"""
        if charset is None:
            charset = string.ascii_lowercase
        length = random.randint(min_len, max_len)
        return ''.join(random.choice(charset) for _ in range(length))
    
    def generate_dictionary_domain(self, min_len: int, max_len: int) -> str:
        """ç”ŸæˆåŸºäºå­—å…¸çš„åŸŸå"""
        words = random.sample(self.dictionary_words, random.randint(1, 3))
        domain = ''.join(words)
        
        # å¦‚æœå¤ªé•¿ï¼Œæˆªæ–­
        if len(domain) > max_len:
            domain = domain[:max_len]
        
        # å¦‚æœå¤ªçŸ­ï¼Œæ·»åŠ éšæœºå­—ç¬¦
        while len(domain) < min_len:
            domain += random.choice(string.ascii_lowercase)
        
        return domain
    
    def generate_mixed_domain(self, min_len: int, max_len: int) -> str:
        """ç”Ÿæˆæ··åˆå‹åŸŸåï¼ˆå­—å…¸+éšæœºï¼‰"""
        # éšæœºé€‰æ‹©1-2ä¸ªå­—å…¸å•è¯
        words = random.sample(self.dictionary_words, random.randint(1, 2))
        base = ''.join(words)
        
        # æ·»åŠ éšæœºå­—ç¬¦æˆ–æ•°å­—
        remaining_len = random.randint(max(0, min_len - len(base)), max_len - len(base))
        if remaining_len > 0:
            charset = string.ascii_lowercase + string.digits
            random_part = ''.join(random.choice(charset) for _ in range(remaining_len))
            
            # éšæœºå†³å®šéšæœºéƒ¨åˆ†çš„ä½ç½®
            if random.choice([True, False]):
                domain = base + random_part
            else:
                domain = random_part + base
        else:
            domain = base
        
        # ç¡®ä¿é•¿åº¦åœ¨èŒƒå›´å†…
        if len(domain) > max_len:
            domain = domain[:max_len]
        while len(domain) < min_len:
            domain += random.choice(string.ascii_lowercase)
        
        return domain
    
    def generate_dga_domain(self, family_name: str, family_config: Dict) -> str:
        """æ ¹æ®DGAå®¶æ—ç”Ÿæˆæ¶æ„åŸŸå"""
        pattern = family_config['pattern']
        min_len = family_config['min_len']
        max_len = family_config['max_len']
        
        if pattern == 'random':
            domain = self.generate_random_string(min_len, max_len)
        elif pattern == 'dictionary':
            domain = self.generate_dictionary_domain(min_len, max_len)
        elif pattern == 'mixed':
            domain = self.generate_mixed_domain(min_len, max_len)
        else:
            domain = self.generate_random_string(min_len, max_len)
        
        # æ·»åŠ TLD
        tld = random.choice(self.tlds)
        full_domain = domain + tld
        
        return full_domain
    
    def generate_benign_domain(self) -> str:
        """ç”Ÿæˆè‰¯æ€§åŸŸå"""
        # é€‰æ‹©ç”Ÿæˆç­–ç•¥
        strategy = random.choice(['company', 'service', 'personal', 'tech'])
        
        if strategy == 'company':
            # å…¬å¸åŸŸåï¼šcompany_name + suffix
            base = random.choice(self.benign_patterns)
            suffix = random.choice(['tech', 'corp', 'inc', 'group', 'solutions'])
            domain = base + suffix
        elif strategy == 'service':
            # æœåŠ¡åŸŸåï¼šservice_type + descriptor
            service = random.choice(['mail', 'web', 'cloud', 'data', 'secure'])
            descriptor = random.choice(['service', 'system', 'platform', 'hub'])
            domain = service + descriptor
        elif strategy == 'personal':
            # ä¸ªäººåŸŸåï¼šname + number/suffix
            name = random.choice(['john', 'mike', 'sarah', 'alex', 'chris', 'david'])
            suffix = random.choice(['blog', 'site', 'home', str(random.randint(1, 999))])
            domain = name + suffix
        else:  # tech
            # æŠ€æœ¯åŸŸåï¼štech_term + suffix
            tech = random.choice(['app', 'dev', 'code', 'soft', 'tech', 'digital'])
            suffix = random.choice(['lab', 'hub', 'zone', 'space', 'world'])
            domain = tech + suffix
        
        # æ·»åŠ TLD
        tld = random.choice(self.tlds)
        full_domain = domain + tld
        
        return full_domain
    
    def build_binary_dataset(self, total_samples: int, benign_ratio: float = 0.5) -> pd.DataFrame:
        """æ„å»ºäºŒåˆ†ç±»æ•°æ®é›†"""
        print(f"æ„å»ºäºŒåˆ†ç±»æ•°æ®é›† (æ€»æ ·æœ¬: {total_samples:,})...")
        
        benign_samples = int(total_samples * benign_ratio)
        malicious_samples = total_samples - benign_samples
        
        domains = []
        labels = []
        families = []
        
        # ç”Ÿæˆè‰¯æ€§åŸŸå
        print(f"ç”Ÿæˆ {benign_samples:,} ä¸ªè‰¯æ€§åŸŸå...")
        for i in range(benign_samples):
            if (i + 1) % 2000 == 0:
                print(f"  å·²ç”Ÿæˆ {i+1:,} ä¸ªè‰¯æ€§åŸŸå")
            
            domain = self.generate_benign_domain()
            domains.append(domain)
            labels.append(0)  # è‰¯æ€§æ ‡ç­¾ä¸º0
            families.append('benign')
        
        # ç”Ÿæˆæ¶æ„åŸŸåï¼ˆéšæœºé€‰æ‹©DGAå®¶æ—ï¼‰
        print(f"ç”Ÿæˆ {malicious_samples:,} ä¸ªæ¶æ„åŸŸå...")
        family_list = list(self.dga_families.keys())
        
        for i in range(malicious_samples):
            if (i + 1) % 2000 == 0:
                print(f"  å·²ç”Ÿæˆ {i+1:,} ä¸ªæ¶æ„åŸŸå")
            
            # éšæœºé€‰æ‹©DGAå®¶æ—
            family = random.choice(family_list)
            family_config = self.dga_families[family]
            
            domain = self.generate_dga_domain(family, family_config)
            domains.append(domain)
            labels.append(1)  # æ¶æ„æ ‡ç­¾ä¸º1
            families.append(family)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame({
            'domain': domains,
            'label': labels,
            'family': families
        })
        
        # æ‰“ä¹±æ•°æ®
        df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(f"äºŒåˆ†ç±»æ•°æ®é›†æ„å»ºå®Œæˆ: {len(df):,} æ ·æœ¬")
        return df
    
    def build_multiclass_dataset(self, total_samples: int, benign_ratio: float = 0.5) -> pd.DataFrame:
        """æ„å»ºå¤šåˆ†ç±»æ•°æ®é›†"""
        print(f"æ„å»ºå¤šåˆ†ç±»æ•°æ®é›† (æ€»æ ·æœ¬: {total_samples:,})...")
        
        benign_samples = int(total_samples * benign_ratio)
        malicious_samples = total_samples - benign_samples
        
        domains = []
        labels = []
        families = []
        
        # ç”Ÿæˆè‰¯æ€§åŸŸå
        print(f"ç”Ÿæˆ {benign_samples:,} ä¸ªè‰¯æ€§åŸŸå...")
        for i in range(benign_samples):
            if (i + 1) % 2000 == 0:
                print(f"  å·²ç”Ÿæˆ {i+1:,} ä¸ªè‰¯æ€§åŸŸå")
            
            domain = self.generate_benign_domain()
            domains.append(domain)
            labels.append(0)  # è‰¯æ€§æ ‡ç­¾ä¸º0
            families.append('benign')
        
        # ç”Ÿæˆæ¶æ„åŸŸåï¼ˆæŒ‰æƒé‡åˆ†é…ï¼‰
        print(f"ç”Ÿæˆ {malicious_samples:,} ä¸ªæ¶æ„åŸŸå...")
        family_list = list(self.dga_families.keys())
        family_weights = [self.dga_families[f]['weight'] for f in family_list]
        
        # æ ¹æ®æƒé‡åˆ†é…æ¯ä¸ªå®¶æ—çš„æ ·æœ¬æ•°
        family_counts = {}
        remaining_samples = malicious_samples
        
        for i, family in enumerate(family_list[:-1]):
            count = int(malicious_samples * family_weights[i])
            family_counts[family] = count
            remaining_samples -= count
        
        # æœ€åä¸€ä¸ªå®¶æ—è·å¾—å‰©ä½™æ ·æœ¬
        family_counts[family_list[-1]] = remaining_samples
        
        print(f"DGAå®¶æ—æ ·æœ¬åˆ†å¸ƒ:")
        for family, count in family_counts.items():
            print(f"  {family}: {count:,} æ ·æœ¬")
        
        # ç”Ÿæˆæ¯ä¸ªå®¶æ—çš„åŸŸå
        label_counter = 1  # ä»1å¼€å§‹ï¼Œ0æ˜¯è‰¯æ€§
        for family, count in family_counts.items():
            family_config = self.dga_families[family]
            
            for i in range(count):
                if (i + 1) % 1000 == 0:
                    print(f"  å·²ç”Ÿæˆ {i+1:,} ä¸ª {family} åŸŸå")
                
                domain = self.generate_dga_domain(family, family_config)
                domains.append(domain)
                labels.append(label_counter)
                families.append(family)
            
            label_counter += 1
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame({
            'domain': domains,
            'label': labels,
            'family': families
        })
        
        # æ‰“ä¹±æ•°æ®
        df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(f"å¤šåˆ†ç±»æ•°æ®é›†æ„å»ºå®Œæˆ: {len(df):,} æ ·æœ¬ï¼Œ{df['label'].nunique()} ç±»åˆ«")
        return df
    
    def split_dataset(self, df: pd.DataFrame, train_ratio: float = 0.8, 
                     val_ratio: float = 0.1, test_ratio: float = 0.1) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """åˆ†å‰²æ•°æ®é›† (8:1:1)"""
        print(f"åˆ†å‰²æ•°æ®é›† (è®­ç»ƒ:{train_ratio*100:.0f}%, éªŒè¯:{val_ratio*100:.0f}%, æµ‹è¯•:{test_ratio*100:.0f}%)...")
        
        # ç¡®ä¿æ¯”ä¾‹æ€»å’Œä¸º1
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6
        
        # åˆ†å±‚æŠ½æ ·ï¼Œç¡®ä¿æ¯ä¸ªç±»åˆ«åœ¨å„ä¸ªé›†åˆä¸­çš„æ¯”ä¾‹ç›¸åŒ
        train_df, temp_df = train_test_split(
            df, test_size=(val_ratio + test_ratio), 
            stratify=df['label'], random_state=42
        )
        
        val_df, test_df = train_test_split(
            temp_df, test_size=test_ratio/(val_ratio + test_ratio),
            stratify=temp_df['label'], random_state=42
        )
        
        print(f"è®­ç»ƒé›†: {len(train_df):,} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_df):,} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(test_df):,} æ ·æœ¬")
        
        return train_df, val_df, test_df
    
    def create_unified_format(self, train_df: pd.DataFrame, val_df: pd.DataFrame, 
                            test_df: pd.DataFrame, dataset_type: str) -> Dict[str, Any]:
        """åˆ›å»ºç»Ÿä¸€æ ¼å¼çš„æ•°æ®é›†"""
        print("åˆ›å»ºç»Ÿä¸€æ ¼å¼æ•°æ®é›†...")
        
        # è·å–æ‰€æœ‰å­—ç¬¦
        all_chars = set()
        for domain in train_df['domain']:
            all_chars.update(domain.lower())
        
        # æ·»åŠ ç‰¹æ®Šå­—ç¬¦
        all_chars.add('<PAD>')  # å¡«å……
        all_chars.add('<UNK>')  # æœªçŸ¥å­—ç¬¦
        
        char_to_idx = {char: idx for idx, char in enumerate(sorted(all_chars))}
        idx_to_char = {idx: char for char, idx in char_to_idx.items()}
        
        # è½¬æ¢åŸŸåä¸ºæ•°å­—åºåˆ—
        def domain_to_sequence(domain: str, max_len: int = 40) -> List[int]:
            sequence = [char_to_idx.get(char.lower(), char_to_idx['<UNK>']) for char in domain]
            # æˆªæ–­æˆ–å¡«å……
            if len(sequence) > max_len:
                sequence = sequence[:max_len]
            else:
                sequence.extend([char_to_idx['<PAD>']] * (max_len - len(sequence)))
            return sequence
        
        # å¤„ç†æ•°æ®
        max_length = 40
        
        train_sequences = [domain_to_sequence(domain, max_length) for domain in train_df['domain']]
        val_sequences = [domain_to_sequence(domain, max_length) for domain in val_df['domain']]
        test_sequences = [domain_to_sequence(domain, max_length) for domain in test_df['domain']]
        
        # æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'total_samples': len(train_df) + len(val_df) + len(test_df),
            'num_classes': train_df['label'].nunique(),
            'vocab_size': len(char_to_idx),
            'max_length': max_length,
            'train_size': len(train_df),
            'val_size': len(val_df),
            'test_size': len(test_df),
            'class_distribution': train_df['label'].value_counts().sort_index().to_dict(),
            'family_distribution': train_df['family'].value_counts().to_dict(),
            'char_to_idx': char_to_idx,
            'idx_to_char': idx_to_char,
            'dataset_type': dataset_type
        }
        
        # ç»Ÿä¸€æ ¼å¼
        unified_dataset = {
            'info': dataset_info,
            'train': {
                'sequences': train_sequences,
                'labels': train_df['label'].tolist(),
                'families': train_df['family'].tolist()
            },
            'val': {
                'sequences': val_sequences,
                'labels': val_df['label'].tolist(),
                'families': val_df['family'].tolist()
            },
            'test': {
                'sequences': test_sequences,
                'labels': test_df['label'].tolist(),
                'families': test_df['family'].tolist()
            }
        }
        
        return unified_dataset
    
    def save_dataset(self, dataset: Dict[str, Any], output_path: str) -> None:
        """ä¿å­˜æ•°æ®é›†"""
        print(f"ä¿å­˜æ•°æ®é›†åˆ° {output_path}...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'wb') as f:
            pickle.dump(dataset, f)
        
        file_size = os.path.getsize(output_path) / 1024 / 1024
        print(f"æ•°æ®é›†å·²ä¿å­˜: {output_path} ({file_size:.2f} MB)")
    
    def build_all_datasets(self) -> None:
        """æ„å»ºæ‰€æœ‰æ•°æ®é›†"""
        print("=" * 80)
        print("ç»Ÿä¸€DGAæ•°æ®é›†æ„å»ºå™¨")
        print("=" * 80)
        
        datasets_config = [
            # (æ ·æœ¬æ•°, ç±»å‹, è¾“å‡ºæ–‡ä»¶å)
            (10000, 'binary', 'small_binary_dga_dataset.pkl'),
            (100000, 'binary', 'large_binary_dga_dataset.pkl'),
            (10000, 'multiclass', 'small_multiclass_dga_dataset.pkl'),
            (100000, 'multiclass', 'large_multiclass_dga_dataset.pkl')
        ]
        
        for total_samples, dataset_type, filename in datasets_config:
            print(f"\n{'='*60}")
            print(f"æ„å»º {dataset_type} æ•°æ®é›† ({total_samples:,} æ ·æœ¬)")
            print(f"{'='*60}")
            
            # æ„å»ºæ•°æ®é›†
            if dataset_type == 'binary':
                df = self.build_binary_dataset(total_samples)
            else:
                df = self.build_multiclass_dataset(total_samples)
            
            # åˆ†å‰²æ•°æ®é›†
            train_df, val_df, test_df = self.split_dataset(df)
            
            # åˆ›å»ºç»Ÿä¸€æ ¼å¼
            unified_dataset = self.create_unified_format(train_df, val_df, test_df, dataset_type)
            
            # ä¿å­˜æ•°æ®é›†
            output_path = f'./data/processed/{filename}'
            self.save_dataset(unified_dataset, output_path)
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            info = unified_dataset['info']
            print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
            print(f"  ç±»å‹: {dataset_type}")
            print(f"  æ€»æ ·æœ¬: {info['total_samples']:,}")
            print(f"  è®­ç»ƒé›†: {info['train_size']:,}")
            print(f"  éªŒè¯é›†: {info['val_size']:,}")
            print(f"  æµ‹è¯•é›†: {info['test_size']:,}")
            print(f"  ç±»åˆ«æ•°: {info['num_classes']}")
            print(f"  è¯æ±‡è¡¨å¤§å°: {info['vocab_size']}")
            print(f"  ç±»åˆ«åˆ†å¸ƒ: {info['class_distribution']}")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºæ„å»ºå™¨
    builder = UnifiedDGADatasetBuilder()
    
    # æ„å»ºæ‰€æœ‰æ•°æ®é›†
    builder.build_all_datasets()
    
    print("\n" + "=" * 80)
    print("æ‰€æœ‰æ•°æ®é›†æ„å»ºå®Œæˆï¼")
    print("=" * 80)
    print("\nç”Ÿæˆçš„æ•°æ®é›†:")
    print("1. small_binary_dga_dataset.pkl - å°å‹äºŒåˆ†ç±»æ•°æ®é›† (1ä¸‡æ ·æœ¬)")
    print("2. large_binary_dga_dataset.pkl - å¤§å‹äºŒåˆ†ç±»æ•°æ®é›† (10ä¸‡æ ·æœ¬)")
    print("3. small_multiclass_dga_dataset.pkl - å°å‹å¤šåˆ†ç±»æ•°æ®é›† (1ä¸‡æ ·æœ¬)")
    print("4. large_multiclass_dga_dataset.pkl - å¤§å‹å¤šåˆ†ç±»æ•°æ®é›† (10ä¸‡æ ·æœ¬)")
    print("\næ‰€æœ‰æ•°æ®é›†é‡‡ç”¨ç»Ÿä¸€æ ¼å¼ï¼Œæ”¯æŒ8:1:1åˆ†å‰²")


if __name__ == '__main__':
    main()
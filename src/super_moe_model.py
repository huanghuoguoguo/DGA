#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DGAæ¶æ„åŸŸåæ£€æµ‹ - åŒ…å«Mambaä¸“å®¶çš„æ‰©å±•MoEæ¶æ„
ä½¿ç”¨CNNã€LSTMã€Transformerå’ŒMambaå››ä¸ªä¸“å®¶çš„æ··åˆä¸“å®¶æ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import pickle
import os
import math
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from tqdm import tqdm
import time

# å¯¼å…¥å„ä¸ªä¸“å®¶æ¨¡å‹çš„ç»„ä»¶
import sys
sys.path.append('./src')

from mamba_model import SelectiveScan, MambaBlock


class DGADataset(Dataset):
    """DGAæ•°æ®é›†ç±»"""
    
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.long)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# CNNä¸“å®¶
class CNNExpert(nn.Module):
    """CNNä¸“å®¶æ¨¡å‹"""
    
    def __init__(self, vocab_size, embed_dim=64, num_classes=2, max_length=40):
        super(CNNExpert, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # å¤šå°ºåº¦å·ç§¯å±‚
        self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=2, padding=1)
        self.conv2 = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(embed_dim, 128, kernel_size=4, padding=2)
        
        self.bn1 = nn.BatchNorm1d(128)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(128)
        
        self.fc = nn.Linear(128 * 3, num_classes)
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x):
        # åµŒå…¥å±‚
        embedded = self.embedding(x)  # (batch, seq, embed_dim)
        embedded = embedded.transpose(1, 2)  # (batch, embed_dim, seq)
        
        # å¤šå°ºåº¦å·ç§¯
        conv1_out = F.relu(self.bn1(self.conv1(embedded)))
        conv2_out = F.relu(self.bn2(self.conv2(embedded)))
        conv3_out = F.relu(self.bn3(self.conv3(embedded)))
        
        # å…¨å±€æœ€å¤§æ± åŒ–
        pool1 = F.max_pool1d(conv1_out, kernel_size=conv1_out.size(2)).squeeze(2)
        pool2 = F.max_pool1d(conv2_out, kernel_size=conv2_out.size(2)).squeeze(2)
        pool3 = F.max_pool1d(conv3_out, kernel_size=conv3_out.size(2)).squeeze(2)
        
        # ç‰¹å¾æ‹¼æ¥
        features = torch.cat([pool1, pool2, pool3], dim=1)
        features = self.dropout(features)
        
        # åˆ†ç±»
        output = self.fc(features)
        return output


# LSTMä¸“å®¶
class LSTMExpert(nn.Module):
    """LSTMä¸“å®¶æ¨¡å‹"""
    
    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128, num_layers=2, num_classes=2, dropout=0.3):
        super(LSTMExpert, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, 
                           dropout=dropout, bidirectional=True, batch_first=True)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Linear(hidden_dim * 2, 1)
        
        # åˆ†ç±»å±‚
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )
        
    def forward(self, x):
        # åµŒå…¥
        embedded = self.embedding(x)  # (batch, seq, embed_dim)
        
        # LSTM
        lstm_out, _ = self.lstm(embedded)  # (batch, seq, hidden_dim*2)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attention_weights = F.softmax(self.attention(lstm_out), dim=1)  # (batch, seq, 1)
        attended = torch.sum(lstm_out * attention_weights, dim=1)  # (batch, hidden_dim*2)
        
        # åˆ†ç±»
        output = self.classifier(attended)
        return output


# Transformerä¸“å®¶ï¼ˆç®€åŒ–ç‰ˆï¼‰
class TransformerExpert(nn.Module):
    """Transformerä¸“å®¶æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, vocab_size, d_model=128, nhead=8, num_layers=2, 
                 num_classes=2, max_len=60, dropout=0.1):
        super(TransformerExpert, self).__init__()
        
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
        
        # ç®€åŒ–çš„Transformer - ä½¿ç”¨PyTorchå†…ç½®çš„TransformerEncoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead, 
            dim_feedforward=d_model*2,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, num_classes)
        )
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # åµŒå…¥
        embedded = self.embedding(x) * math.sqrt(self.d_model)  # (batch, seq, d_model)
        embedded = self.dropout(embedded)
        
        # Transformerç¼–ç 
        transformer_out = self.transformer(embedded)  # (batch, seq, d_model)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        pooled = torch.mean(transformer_out, dim=1)  # (batch, d_model)
        
        # åˆ†ç±»
        output = self.classifier(pooled)
        return output


# Mambaä¸“å®¶
class MambaExpert(nn.Module):
    """Mambaä¸“å®¶æ¨¡å‹"""
    
    def __init__(self, vocab_size, d_model=256, n_layers=2, d_state=16, 
                 d_conv=4, expand_factor=2, num_classes=2, dropout=0.1):
        super(MambaExpert, self).__init__()
        
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
        
        # Mambaå±‚
        self.layers = nn.ModuleList([
            MambaBlock(d_model, d_state, d_conv, expand_factor, dropout)
            for _ in range(n_layers)
        ])
        
        self.norm = nn.LayerNorm(d_model)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, num_classes)
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # åµŒå…¥
        x = self.embedding(x)  # (batch, seq_len, d_model)
        x = self.dropout(x)
        
        # Mambaå±‚
        for layer in self.layers:
            x = layer(x)
        
        x = self.norm(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = torch.mean(x, dim=1)  # (batch, d_model)
        
        # åˆ†ç±»
        logits = self.classifier(x)
        return logits


# é—¨æ§ç½‘ç»œ
class AdvancedGatingNetwork(nn.Module):
    """é«˜çº§é—¨æ§ç½‘ç»œ - æ”¯æŒå››ä¸ªä¸“å®¶"""
    
    def __init__(self, vocab_size, embed_dim=64, num_experts=4, dropout=0.1):
        super(AdvancedGatingNetwork, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # å¤šå±‚ç‰¹å¾æå–
        self.feature_extractor = nn.Sequential(
            nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # é—¨æ§å†³ç­–ç½‘ç»œ
        self.gate = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, num_experts)
        )
        
    def forward(self, x):
        # x: (batch, seq_len) - åŸå§‹è¾“å…¥åºåˆ—
        embedded = self.embedding(x).transpose(1, 2)  # (batch, embed_dim, seq_len)
        features = self.feature_extractor(embedded)  # (batch, 256)
        gate_logits = self.gate(features)  # (batch, num_experts)
        gate_weights = F.softmax(gate_logits, dim=1)
        return gate_weights


# è¶…çº§MoEæ¨¡å‹
class SuperMoEModel(nn.Module):
    """è¶…çº§ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆåŒ…å«CNNã€LSTMã€Transformerã€Mambaå››ä¸ªä¸“å®¶ï¼‰"""
    
    def __init__(self, vocab_size, embed_dim=64, num_classes=2, max_length=40):
        super(SuperMoEModel, self).__init__()
        
        self.num_experts = 4
        self.vocab_size = vocab_size
        
        # åˆ›å»ºå››ä¸ªä¸“å®¶
        self.cnn_expert = CNNExpert(vocab_size, embed_dim, num_classes, max_length)
        self.lstm_expert = LSTMExpert(vocab_size, embed_dim, 128, 2, num_classes, 0.3)
        self.transformer_expert = TransformerExpert(vocab_size, 128, 8, 2, num_classes, max_length, 0.1)
        self.mamba_expert = MambaExpert(vocab_size, 256, 2, 16, 4, 2, num_classes, 0.1)
        
        # é«˜çº§é—¨æ§ç½‘ç»œ
        self.gating_network = AdvancedGatingNetwork(vocab_size, embed_dim, self.num_experts)
        
        # è´Ÿè½½å‡è¡¡æŸå¤±çš„æƒé‡
        self.load_balance_weight = 0.01
        
    def forward(self, x, return_expert_outputs=False):
        batch_size = x.size(0)
        
        # è·å–é—¨æ§æƒé‡
        gate_weights = self.gating_network(x)  # (batch, 4)
        
        # è·å–å„ä¸“å®¶çš„è¾“å‡º
        cnn_output = self.cnn_expert(x)
        lstm_output = self.lstm_expert(x)
        transformer_output = self.transformer_expert(x)
        mamba_output = self.mamba_expert(x)
        
        # ä¸“å®¶è¾“å‡ºå †å 
        expert_outputs = torch.stack([cnn_output, lstm_output, transformer_output, mamba_output], dim=2)  # (batch, num_classes, num_experts)
        
        # æ ¹æ®é—¨æ§æƒé‡åŠ æƒæ±‚å’Œ
        gate_weights = gate_weights.unsqueeze(1)  # (batch, 1, num_experts)
        final_output = torch.sum(expert_outputs * gate_weights, dim=2)  # (batch, num_classes)
        
        if return_expert_outputs:
            return final_output, gate_weights.squeeze(1), expert_outputs
        
        return final_output, gate_weights.squeeze(1)
    
    def load_balance_loss(self, gate_weights):
        """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±"""
        # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰æ‹©çš„å¹³å‡æ¦‚ç‡
        expert_usage = torch.mean(gate_weights, dim=0)  # (num_experts,)
        
        # ç†æƒ³æƒ…å†µä¸‹æ¯ä¸ªä¸“å®¶è¢«é€‰æ‹©çš„æ¦‚ç‡åº”è¯¥ç›¸ç­‰
        target_usage = 1.0 / self.num_experts
        
        # è®¡ç®—ä¸å¹³è¡¡ç¨‹åº¦
        load_loss = torch.sum((expert_usage - target_usage) ** 2)
        
        return load_loss
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        # å„ä¸“å®¶å‚æ•°é‡
        cnn_params = sum(p.numel() for p in self.cnn_expert.parameters())
        lstm_params = sum(p.numel() for p in self.lstm_expert.parameters())
        transformer_params = sum(p.numel() for p in self.transformer_expert.parameters())
        mamba_params = sum(p.numel() for p in self.mamba_expert.parameters())
        gate_params = sum(p.numel() for p in self.gating_network.parameters())
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'model_size_mb': total_params * 4 / 1024 / 1024,
            'num_experts': self.num_experts,
            'cnn_params': cnn_params,
            'lstm_params': lstm_params,
            'transformer_params': transformer_params,
            'mamba_params': mamba_params,
            'gate_params': gate_params
        }


def load_dataset(file_path='./data/small_dga_dataset.pkl'):
    """åŠ è½½æ•°æ®é›†"""
    with open(file_path, 'rb') as f:
        dataset = pickle.load(f)
    return dataset


def main():
    """ä¸»å‡½æ•° - è®­ç»ƒè¶…çº§MoEæ¨¡å‹"""
    print("=== DGAæ¶æ„åŸŸåæ£€æµ‹ - è¶…çº§MoEæ¶æ„è®­ç»ƒï¼ˆCNN+LSTM+Transformer+Mambaï¼‰ ===\\n")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®é›†
    print("åŠ è½½æ•°æ®é›†...")
    dataset = load_dataset('./data/small_dga_dataset.pkl')
    X, y = dataset['X'], dataset['y']
    vocab_size = dataset['vocab_size']
    
    # åˆ›å»ºæ•°æ®é›†
    full_dataset = DGADataset(X, y)
    
    # æ•°æ®é›†åˆ’åˆ†
    total_size = len(full_dataset)
    train_size = int(0.7 * total_size)
    val_size = int(0.15 * total_size)
    test_size = total_size - train_size - val_size
    
    train_dataset, val_dataset, test_dataset = random_split(
        full_dataset, [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = 32
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # åˆ›å»ºè¶…çº§MoEæ¨¡å‹
    model = SuperMoEModel(vocab_size=vocab_size, embed_dim=64, num_classes=2, max_length=40)
    model.to(device)
    
    model_info = model.get_model_info()
    print(f"è¶…çº§MoEæ¨¡å‹å‚æ•°é‡: {model_info['total_params']:,}")
    print(f"CNNä¸“å®¶: {model_info['cnn_params']:,}, LSTMä¸“å®¶: {model_info['lstm_params']:,}")
    print(f"Transformerä¸“å®¶: {model_info['transformer_params']:,}, Mambaä¸“å®¶: {model_info['mamba_params']:,}")
    print(f"é—¨æ§ç½‘ç»œ: {model_info['gate_params']:,}")
    
    # è®­ç»ƒå‚æ•°
    num_epochs = 25
    learning_rate = 0.001
    
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)
    
    # è®­ç»ƒå¾ªç¯
    best_val_acc = 0
    patience = 10
    patience_counter = 0
    
    print("å¼€å§‹è®­ç»ƒè¶…çº§MoEæ¨¡å‹...")
    
    for epoch in range(1, num_epochs + 1):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        epoch_gate_weights = []
        
        for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch} Training")):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output, gate_weights = model(data)
            
            # åˆ†ç±»æŸå¤± + è´Ÿè½½å‡è¡¡æŸå¤±
            classification_loss = criterion(output, target)
            load_balance_loss = model.load_balance_loss(gate_weights)
            total_loss = classification_loss + model.load_balance_weight * load_balance_loss
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += total_loss.item()
            pred = output.argmax(dim=1)
            train_correct += pred.eq(target).sum().item()
            train_total += target.size(0)
            
            epoch_gate_weights.append(gate_weights.detach().cpu())
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        val_gate_weights = []
        
        with torch.no_grad():
            for data, target in tqdm(val_loader, desc=f"Epoch {epoch} Validation"):
                data, target = data.to(device), target.to(device)
                output, gate_weights = model(data)
                
                classification_loss = criterion(output, target)
                load_balance_loss = model.load_balance_loss(gate_weights)
                total_loss = classification_loss + model.load_balance_weight * load_balance_loss
                
                val_loss += total_loss.item()
                pred = output.argmax(dim=1)
                val_correct += pred.eq(target).sum().item()
                val_total += target.size(0)
                
                val_gate_weights.append(gate_weights.cpu())
        
        # è®¡ç®—æŒ‡æ ‡
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        # ä¸“å®¶ä½¿ç”¨ç‡
        train_expert_usage = torch.mean(torch.cat(epoch_gate_weights, dim=0), dim=0)
        val_expert_usage = torch.mean(torch.cat(val_gate_weights, dim=0), dim=0)
        
        print(f"Epoch {epoch}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")
        print(f"Expert Usage - CNN: {train_expert_usage[0]:.3f}, LSTM: {train_expert_usage[1]:.3f}, "
              f"Transformer: {train_expert_usage[2]:.3f}, Mamba: {train_expert_usage[3]:.3f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            torch.save(model.state_dict(), './data/best_super_moe_model.pth')
            print(f"æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%ï¼Œæ¨¡å‹å·²ä¿å­˜")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"éªŒè¯å‡†ç¡®ç‡è¿ç»­{patience}ä¸ªepochæœªæå‡ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                break
        
        scheduler.step(val_acc)
    
    # æµ‹è¯•è¯„ä¼°
    model.load_state_dict(torch.load('./data/best_super_moe_model.pth'))
    print(f"\\nåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³è¶…çº§MoEæ¨¡å‹...")
    
    model.eval()
    test_correct = 0
    test_total = 0
    test_gate_weights = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output, gate_weights = model(data)
            
            pred = output.argmax(dim=1)
            test_correct += pred.eq(target).sum().item()
            test_total += target.size(0)
            
            test_gate_weights.append(gate_weights.cpu())
    
    test_acc = 100. * test_correct / test_total
    test_expert_usage = torch.mean(torch.cat(test_gate_weights, dim=0), dim=0)
    
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
    print(f"æµ‹è¯•é›†ä¸“å®¶ä½¿ç”¨ç‡ - CNN: {test_expert_usage[0]:.3f}, LSTM: {test_expert_usage[1]:.3f}, "
          f"Transformer: {test_expert_usage[2]:.3f}, Mamba: {test_expert_usage[3]:.3f}")
    
    # ä¿å­˜ç»“æœ
    results = {
        'model_name': 'Super MoE (CNN+LSTM+Transformer+Mamba)',
        'model_info': model_info,
        'best_val_accuracy': best_val_acc,
        'test_accuracy': test_acc / 100,
        'test_expert_usage': {
            'cnn': test_expert_usage[0].item(),
            'lstm': test_expert_usage[1].item(),
            'transformer': test_expert_usage[2].item(),
            'mamba': test_expert_usage[3].item()
        }
    }
    
    with open('./data/super_moe_results.pkl', 'wb') as f:
        pickle.dump(results, f)
    
    print(f"\\nğŸ‰ è¶…çº§MoEè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
    print(f"ç»“æœå·²ä¿å­˜åˆ° ./data/super_moe_results.pkl")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DGAæ¶æ„åŸŸåæ£€æµ‹ - å¤§è§„æ¨¡æ•°æ®é›†æ„å»ºå™¨
ç”¨äºæ„å»ºæ›´å¤§è§„æ¨¡çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ï¼ŒéªŒè¯æ•°æ®é‡å¯¹MoEæ¨¡å‹æ€§èƒ½çš„å½±å“
"""

import os
import pandas as pd
import numpy as np
from typing import List, Tuple, Dict
import random
from collections import Counter
import pickle
import math

class LargeDGADatasetBuilder:
    """å¤§è§„æ¨¡DGAæ•°æ®é›†æ„å»ºå™¨"""
    
    def __init__(self, data_dir: str = "./data", max_length: int = 40):
        """
        åˆå§‹åŒ–æ•°æ®é›†æ„å»ºå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            max_length: åŸŸåæœ€å¤§é•¿åº¦
        """
        self.data_dir = data_dir
        self.max_length = max_length
        self.char_to_idx = {}  # å­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„
        self.idx_to_char = {}  # ç´¢å¼•åˆ°å­—ç¬¦çš„æ˜ å°„
        self.vocab_size = 0
        
        # åˆå§‹åŒ–å­—ç¬¦æ˜ å°„
        self._build_char_mapping()
        
        # å¯ç”¨çš„DGAå®¶æ—åˆ—è¡¨
        self.available_families = [
            'cryptolocker-50000', 'necurs-50000', 'locky-50000', 'ramnit-50000',
            'bamital-50000', 'banjori-50000', 'bazarbackdoor-50000', 'bedep-50000',
            'bigviktor-50000', 'chinad-50000', 'corebot-50000', 'dircrypt-50000',
            'dnschanger-50000', 'dyre-50000', 'emotet-50000', 'enviserv-50000',
            'fobber_v1-50000', 'gozi_gpl-50000', 'kraken_v1-50000', 'matsnu-50000',
            'monerodownloader-50000', 'murofet_v1-50000', 'murofetweekly-50000',
            'mydoom-50000', 'newgoz-50000', 'nymaim2-50000', 'padcrypt-50000',
            'pandabanker-20000', 'pitou-50000', 'proslikefan-50000', 'pushdo-50000',
            'pykspa_improved_useful-50000', 'pykspa_precursor-50000', 'qadars-50000',
            'qakbot-50000', 'qsnatch-50000', 'ramdo-50000', 'ranbyus_v1-50000',
            'reconyc-50000', 'rovnix-50000', 'shiotob-50000', 'simda-50000',
            'sisron-50000', 'sphinx-50000', 'suppobox_1-20000', 'symmi-50000',
            'tempedreve-50000', 'tinba-50000', 'tinynuke-50000', 'torpig-50000',
            'vawtrak_v1-50000', 'vidro-50000', 'virut-50000', 'wd-20000', 'zloader-50000'
        ]
        
    def _build_char_mapping(self):
        """æ„å»ºå­—ç¬¦æ˜ å°„è¡¨"""
        # åŸºç¡€å­—ç¬¦é›†ï¼ša-z, 0-9, ç‰¹æ®Šå­—ç¬¦
        chars = ['<PAD>', '<UNK>'] + list('abcdefghijklmnopqrstuvwxyz0123456789-_.')
        
        self.char_to_idx = {char: idx for idx, char in enumerate(chars)}
        self.idx_to_char = {idx: char for idx, char in enumerate(chars)}
        self.vocab_size = len(chars)
        
        print(f"å­—ç¬¦æ˜ å°„è¡¨æ„å»ºå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
    
    def load_dga_data(self, family_name: str, num_samples: int = 1000) -> List[str]:
        """
        åŠ è½½æŒ‡å®šDGAå®¶æ—çš„æ•°æ®
        
        Args:
            family_name: DGAå®¶æ—åç§°
            num_samples: é‡‡æ ·æ•°é‡
            
        Returns:
            åŸŸååˆ—è¡¨
        """
        file_path = os.path.join(self.data_dir, "DGA_Botnets_Domains", f"{family_name}.txt")
        
        if not os.path.exists(file_path):
            print(f"è­¦å‘Š: æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
            return []
        
        domains = []
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
                # éšæœºé‡‡æ ·
                if len(lines) > num_samples:
                    lines = random.sample(lines, num_samples)
                
                for line in lines:
                    domain = line.strip().lower()
                    # å»é™¤é¡¶çº§åŸŸååç¼€
                    if '.' in domain:
                        domain = domain.split('.')[0]
                    
                    # è¿‡æ»¤é•¿åº¦å’Œå­—ç¬¦
                    if 3 <= len(domain) <= self.max_length and self._is_valid_domain(domain):
                        domains.append(domain)
        
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        
        print(f"ä» {family_name} åŠ è½½äº† {len(domains)} ä¸ªåŸŸå")
        return domains
    
    def _is_valid_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦æœ‰æ•ˆ"""
        # åªåŒ…å«å…è®¸çš„å­—ç¬¦
        allowed_chars = set('abcdefghijklmnopqrstuvwxyz0123456789-_.')
        return all(c in allowed_chars for c in domain)
    
    def load_benign_data(self, num_samples: int = 1000) -> List[str]:
        """
        åŠ è½½è‰¯æ€§åŸŸåæ•°æ®
        
        Args:
            num_samples: é‡‡æ ·æ•°é‡
            
        Returns:
            è‰¯æ€§åŸŸååˆ—è¡¨
        """
        benign_files = [
            os.path.join(self.data_dir, "DGA_Botnets_Domains", "legit-1000000.txt"),
            os.path.join(self.data_dir, "alexa_1000000"),
            os.path.join(self.data_dir, "alexa_2019"),
            os.path.join(self.data_dir, "top-1m.txt")
        ]
        
        domains = []
        samples_per_file = num_samples // len([f for f in benign_files if os.path.exists(f)])
        
        for file_path in benign_files:
            if not os.path.exists(file_path):
                continue
                
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()
                    if len(lines) > samples_per_file:
                        lines = random.sample(lines, samples_per_file)
                    
                    for line in lines:
                        domain = line.strip().lower()
                        # å¤„ç†ä¸åŒæ ¼å¼çš„æ–‡ä»¶
                        if ',' in domain:  # Alexaæ ¼å¼: rank,domain
                            domain = domain.split(',')[1] if len(domain.split(',')) > 1 else domain.split(',')[0]
                        
                        # å»é™¤é¡¶çº§åŸŸååç¼€
                        if '.' in domain:
                            domain = domain.split('.')[0]
                        
                        # è¿‡æ»¤é•¿åº¦å’Œå­—ç¬¦
                        if 3 <= len(domain) <= self.max_length and self._is_valid_domain(domain):
                            domains.append(domain)
                            
                        if len(domains) >= num_samples:
                            break
                            
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        # å¦‚æœæ ·æœ¬ä¸è¶³ï¼Œéšæœºé‡å¤ä¸€äº›
        while len(domains) < num_samples and domains:
            domains.extend(random.sample(domains, min(len(domains), num_samples - len(domains))))
        
        domains = domains[:num_samples]  # ç¡®ä¿ä¸è¶…è¿‡è¦æ±‚æ•°é‡
        print(f"åŠ è½½äº† {len(domains)} ä¸ªè‰¯æ€§åŸŸå")
        return domains
    
    def encode_domain(self, domain: str) -> List[int]:
        """
        å°†åŸŸåç¼–ç ä¸ºæ•´æ•°åºåˆ—
        
        Args:
            domain: åŸŸåå­—ç¬¦ä¸²
            
        Returns:
            ç¼–ç åçš„æ•´æ•°åˆ—è¡¨
        """
        encoded = []
        for char in domain:
            if char in self.char_to_idx:
                encoded.append(self.char_to_idx[char])
            else:
                encoded.append(self.char_to_idx['<UNK>'])  # æœªçŸ¥å­—ç¬¦
        
        # å¡«å……æˆ–æˆªæ–­åˆ°å›ºå®šé•¿åº¦
        if len(encoded) < self.max_length:
            encoded.extend([self.char_to_idx['<PAD>']] * (self.max_length - len(encoded)))
        else:
            encoded = encoded[:self.max_length]
        
        return encoded
    
    def build_large_dataset(self, 
                           target_size: int = 20000,
                           dga_ratio: float = 0.5) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†
        
        Args:
            target_size: ç›®æ ‡æ•°æ®é›†å¤§å°
            dga_ratio: DGAæ ·æœ¬æ¯”ä¾‹
            
        Returns:
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾æ•°ç»„  
            families: å®¶æ—åç§°åˆ—è¡¨
        """
        dga_samples = int(target_size * dga_ratio)
        benign_samples = target_size - dga_samples
        
        print(f"æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†: æ€»æ ·æœ¬{target_size}, DGAæ ·æœ¬{dga_samples}, è‰¯æ€§æ ·æœ¬{benign_samples}")
        
        all_domains = []
        all_labels = []
        family_labels = []
        
        # åŠ è½½è‰¯æ€§åŸŸå
        print("\nåŠ è½½è‰¯æ€§åŸŸå...")
        benign_domains = self.load_benign_data(benign_samples)
        all_domains.extend(benign_domains)
        all_labels.extend([0] * len(benign_domains))
        family_labels.extend(['benign'] * len(benign_domains))
        
        # è®¡ç®—æ¯ä¸ªDGAå®¶æ—éœ€è¦çš„æ ·æœ¬æ•°
        available_families = [f for f in self.available_families 
                            if os.path.exists(os.path.join(self.data_dir, "DGA_Botnets_Domains", f"{f}.txt"))]
        
        samples_per_family = max(1, dga_samples // len(available_families))
        remaining_samples = dga_samples - (samples_per_family * len(available_families))
        
        print(f"\nå¯ç”¨DGAå®¶æ—: {len(available_families)}")
        print(f"æ¯ä¸ªå®¶æ—æ ·æœ¬æ•°: {samples_per_family}")
        
        # åŠ è½½DGAåŸŸå
        for i, family in enumerate(available_families):
            # ä¸ºå‰å‡ ä¸ªå®¶æ—åˆ†é…é¢å¤–çš„æ ·æœ¬
            current_samples = samples_per_family
            if i < remaining_samples:
                current_samples += 1
                
            print(f"åŠ è½½DGAå®¶æ—: {family} ({current_samples}ä¸ªæ ·æœ¬)")
            dga_domains = self.load_dga_data(family, current_samples)
            
            all_domains.extend(dga_domains)
            all_labels.extend([1] * len(dga_domains))  # DGAæ ‡ç­¾ä¸º1
            family_labels.extend([family] * len(dga_domains))
        
        print(f"\næ•°æ®é›†ç»Ÿè®¡:")
        print(f"æ€»æ ·æœ¬æ•°: {len(all_domains)}")
        print(f"è‰¯æ€§æ ·æœ¬: {sum(1 for x in all_labels if x == 0)}")
        print(f"æ¶æ„æ ·æœ¬: {sum(1 for x in all_labels if x == 1)}")
        
        # ç¼–ç åŸŸå
        print("\nç¼–ç åŸŸå...")
        X = []
        valid_indices = []
        
        for i, domain in enumerate(all_domains):
            if domain:  # ç¡®ä¿åŸŸåéç©º
                encoded = self.encode_domain(domain)
                X.append(encoded)
                valid_indices.append(i)
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        X = np.array(X)
        y = np.array([all_labels[i] for i in valid_indices])
        families = [family_labels[i] for i in valid_indices]
        
        print(f"æœ€ç»ˆæ•°æ®é›†å½¢çŠ¶: X={X.shape}, y={y.shape}")
        return X, y, families
    
    def analyze_large_dataset(self, domains: List[str], labels: List[int], families: List[str]):
        """åˆ†æå¤§è§„æ¨¡æ•°æ®é›†çš„ç‰¹å¾"""
        print("\n=== å¤§è§„æ¨¡æ•°æ®é›†åˆ†æ ===")
        
        # åŸºæœ¬ç»Ÿè®¡
        total_samples = len(domains)
        benign_count = sum(1 for label in labels if label == 0)
        dga_count = sum(1 for label in labels if label == 1)
        
        print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"è‰¯æ€§æ ·æœ¬: {benign_count} ({benign_count/total_samples*100:.1f}%)")
        print(f"æ¶æ„æ ·æœ¬: {dga_count} ({dga_count/total_samples*100:.1f}%)")
        
        # é•¿åº¦åˆ†æ
        lengths = [len(domain) for domain in domains]
        print(f"\nåŸŸåé•¿åº¦ç»Ÿè®¡:")
        print(f"å¹³å‡é•¿åº¦: {np.mean(lengths):.2f}")
        print(f"æœ€çŸ­é•¿åº¦: {min(lengths)}")
        print(f"æœ€é•¿é•¿åº¦: {max(lengths)}")
        print(f"é•¿åº¦æ ‡å‡†å·®: {np.std(lengths):.2f}")
        
        # DGAå®¶æ—åˆ†æ
        dga_families = [f for f, label in zip(families, labels) if label == 1 and f != 'benign']
        family_counts = Counter(dga_families)
        
        print(f"\nDGAå®¶æ—åˆ†å¸ƒ (å‰10):")
        for family, count in family_counts.most_common(10):
            print(f"  {family}: {count}ä¸ªæ ·æœ¬")
        
        print(f"æ€»DGAå®¶æ—æ•°: {len(family_counts)}")
        
        # å­—ç¬¦åˆ†æ
        all_chars = set(''.join(domains))
        print(f"\nå­—ç¬¦é›†åˆ†æ:")
        print(f"å”¯ä¸€å­—ç¬¦æ•°: {len(all_chars)}")
        print(f"å­—ç¬¦é›†: {''.join(sorted(all_chars))}")
        
        # ç†µåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        def calculate_entropy(domain):
            char_counts = Counter(domain)
            total_chars = len(domain)
            entropy = 0
            for count in char_counts.values():
                p = count / total_chars
                entropy -= p * math.log2(p)
            return entropy
        
        benign_entropies = [calculate_entropy(domain) for domain, label in zip(domains, labels) if label == 0]
        dga_entropies = [calculate_entropy(domain) for domain, label in zip(domains, labels) if label == 1]
        
        print(f"\nç†µåˆ†æ:")
        print(f"è‰¯æ€§åŸŸåå¹³å‡ç†µ: {np.mean(benign_entropies):.3f}")
        print(f"DGAåŸŸåå¹³å‡ç†µ: {np.mean(dga_entropies):.3f}")
        print(f"ç†µå·®å¼‚: {np.mean(dga_entropies) - np.mean(benign_entropies):.3f}")
    
    def save_dataset(self, X: np.ndarray, y: np.ndarray, families: List[str], 
                    file_path: str = "./data/large_dga_dataset.pkl"):
        """ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶"""
        dataset = {
            'X': X,
            'y': y,
            'families': families,
            'vocab_size': self.vocab_size,
            'char_to_idx': self.char_to_idx,
            'idx_to_char': self.idx_to_char,
            'max_length': self.max_length
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        with open(file_path, 'wb') as f:
            pickle.dump(dataset, f)
        
        print(f"\næ•°æ®é›†å·²ä¿å­˜åˆ°: {file_path}")
        print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(file_path) / 1024 / 1024:.2f} MB")
    
    def load_dataset(self, file_path: str = "./data/large_dga_dataset.pkl") -> Dict:
        """ä»æ–‡ä»¶åŠ è½½æ•°æ®é›†"""
        with open(file_path, 'rb') as f:
            dataset = pickle.load(f)
        return dataset


def main():
    """ä¸»å‡½æ•° - æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†"""
    print("=== DGAæ¶æ„åŸŸåæ£€æµ‹ - å¤§è§„æ¨¡æ•°æ®é›†æ„å»º ===\n")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºæ•°æ®é›†æ„å»ºå™¨
    builder = LargeDGADatasetBuilder(data_dir="./data", max_length=40)
    
    # æ„å»ºä¸åŒè§„æ¨¡çš„æ•°æ®é›†
    dataset_configs = [
        {"name": "ä¸­ç­‰è§„æ¨¡", "size": 10000, "filename": "medium_dga_dataset.pkl"},
        {"name": "å¤§è§„æ¨¡", "size": 20000, "filename": "large_dga_dataset.pkl"},
        {"name": "è¶…å¤§è§„æ¨¡", "size": 50000, "filename": "xlarge_dga_dataset.pkl"}
    ]
    
    for config in dataset_configs:
        print(f"\n{'='*60}")
        print(f"æ„å»º{config['name']}æ•°æ®é›† ({config['size']}æ ·æœ¬)")
        print(f"{'='*60}")
        
        try:
            X, y, families = builder.build_large_dataset(
                target_size=config['size'],
                dga_ratio=0.5  # ä¿æŒå¹³è¡¡
            )
            
            # åˆ†ææ•°æ®é›†
            domains_for_analysis = []
            for i in range(min(1000, len(X))):
                domain = ''.join([builder.idx_to_char.get(idx, '') for idx in X[i] 
                                if idx != builder.char_to_idx['<PAD>']])
                domains_for_analysis.append(domain)
            
            builder.analyze_large_dataset(
                domains_for_analysis, 
                y[:len(domains_for_analysis)].tolist(),
                families[:len(domains_for_analysis)]
            )
            
            # ä¿å­˜æ•°æ®é›†
            save_path = f"./data/processed/{config['filename']}"
            builder.save_dataset(X, y, families, save_path)
            
            print(f"\nâœ… {config['name']}æ•°æ®é›†æ„å»ºå®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ æ„å»º{config['name']}æ•°æ®é›†æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®é›†æ„å»ºå®Œæˆï¼")
    print(f"\nğŸ“ ä½¿ç”¨æ–¹æ³•:")
    print(f"  python simple_train.py --model simplified_moe --dataset medium")
    print(f"  python simple_train.py --model simplified_moe --dataset large")
    print(f"  python simple_train.py --model simplified_moe --dataset xlarge")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DGAæ¶æ„åŸŸåæ£€æµ‹ - æ‰©å±•MoEæ¶æ„ï¼ˆåŒ…å«Transformerä¸“å®¶ï¼‰
ä½¿ç”¨CNNã€LSTMå’ŒTransformerä¸‰ä¸ªä¸“å®¶çš„æ··åˆä¸“å®¶æ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import pickle
import os
import math
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from tqdm import tqdm
import time


class DGADataset(Dataset):
    """DGAæ•°æ®é›†ç±»"""
    
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.long)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# CNNä¸“å®¶
class CNNExpert(nn.Module):
    """CNNä¸“å®¶æ¨¡å‹"""
    
    def __init__(self, vocab_size, embed_dim=64, num_classes=2, max_length=40):
        super(CNNExpert, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # å¤šå°ºåº¦å·ç§¯å±‚
        self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=2, padding=1)
        self.conv2 = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(embed_dim, 128, kernel_size=4, padding=2)
        
        self.bn1 = nn.BatchNorm1d(128)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(128)
        
        self.fc = nn.Linear(128 * 3, num_classes)
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x):
        # åµŒå…¥å±‚
        embedded = self.embedding(x)  # (batch, seq, embed_dim)
        embedded = embedded.transpose(1, 2)  # (batch, embed_dim, seq)
        
        # å¤šå°ºåº¦å·ç§¯
        conv1_out = F.relu(self.bn1(self.conv1(embedded)))
        conv2_out = F.relu(self.bn2(self.conv2(embedded)))
        conv3_out = F.relu(self.bn3(self.conv3(embedded)))
        
        # å…¨å±€æœ€å¤§æ± åŒ–
        pool1 = F.max_pool1d(conv1_out, kernel_size=conv1_out.size(2)).squeeze(2)
        pool2 = F.max_pool1d(conv2_out, kernel_size=conv2_out.size(2)).squeeze(2)
        pool3 = F.max_pool1d(conv3_out, kernel_size=conv3_out.size(2)).squeeze(2)
        
        # ç‰¹å¾æ‹¼æ¥
        features = torch.cat([pool1, pool2, pool3], dim=1)
        features = self.dropout(features)
        
        # åˆ†ç±»
        output = self.fc(features)
        return output


# LSTMä¸“å®¶
class LSTMExpert(nn.Module):
    """LSTMä¸“å®¶æ¨¡å‹"""
    
    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128, num_layers=2, num_classes=2, dropout=0.3):
        super(LSTMExpert, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, 
                           dropout=dropout, bidirectional=True, batch_first=True)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Linear(hidden_dim * 2, 1)
        
        # åˆ†ç±»å±‚
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )
        
    def forward(self, x):
        # åµŒå…¥
        embedded = self.embedding(x)  # (batch, seq, embed_dim)
        
        # LSTM
        lstm_out, _ = self.lstm(embedded)  # (batch, seq, hidden_dim*2)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attention_weights = F.softmax(self.attention(lstm_out), dim=1)  # (batch, seq, 1)
        attended = torch.sum(lstm_out * attention_weights, dim=1)  # (batch, hidden_dim*2)
        
        # åˆ†ç±»
        output = self.classifier(attended)
        return output


# Transformerä¸“å®¶ï¼ˆç®€åŒ–ç‰ˆï¼‰
class TransformerExpert(nn.Module):
    """Transformerä¸“å®¶æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, vocab_size, d_model=128, nhead=8, num_layers=2, 
                 num_classes=2, max_len=60, dropout=0.1):
        super(TransformerExpert, self).__init__()
        
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
        
        # ç®€åŒ–çš„Transformer - ä½¿ç”¨PyTorchå†…ç½®çš„TransformerEncoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead, 
            dim_feedforward=d_model*2,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, num_classes)
        )
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # åµŒå…¥
        embedded = self.embedding(x) * math.sqrt(self.d_model)  # (batch, seq, d_model)
        embedded = self.dropout(embedded)
        
        # Transformerç¼–ç 
        transformer_out = self.transformer(embedded)  # (batch, seq, d_model)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        pooled = torch.mean(transformer_out, dim=1)  # (batch, d_model)
        
        # åˆ†ç±»
        output = self.classifier(pooled)
        return output


# é—¨æ§ç½‘ç»œ
class GatingNetwork(nn.Module):
    """é—¨æ§ç½‘ç»œ - å†³å®šä½¿ç”¨å“ªä¸ªä¸“å®¶"""
    
    def __init__(self, input_dim, num_experts=3, dropout=0.1):
        super(GatingNetwork, self).__init__()
        
        self.gate = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, num_experts)
        )
        
    def forward(self, x):
        # x: (batch, seq_len) - åŸå§‹è¾“å…¥åºåˆ—
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ç‰¹å¾ä½œä¸ºé—¨æ§è¾“å…¥
        batch_size = x.size(0)
        
        # ç»Ÿè®¡ç‰¹å¾
        seq_len = (x != 0).sum(dim=1).float()  # æœ‰æ•ˆé•¿åº¦
        unique_chars = torch.tensor([len(torch.unique(sample[sample != 0])) for sample in x], 
                                  dtype=torch.float, device=x.device)  # å”¯ä¸€å­—ç¬¦æ•°
        
        # å­—ç¬¦é¢‘ç‡ç‰¹å¾
        char_freq = torch.zeros(batch_size, 40, device=x.device)  # å‡è®¾vocab_size=40
        for i in range(batch_size):
            chars, counts = torch.unique(x[i][x[i] != 0], return_counts=True)
            char_freq[i][chars] = counts.float()
        
        # æ‹¼æ¥ç‰¹å¾
        features = torch.cat([
            seq_len.unsqueeze(1),
            unique_chars.unsqueeze(1),
            char_freq
        ], dim=1)  # (batch, 42)
        
        # é—¨æ§å†³ç­–
        gate_logits = self.gate(features)  # (batch, num_experts)
        gate_weights = F.softmax(gate_logits, dim=1)
        
        return gate_weights


# æ‰©å±•MoEæ¨¡å‹
class ExtendedMoEModel(nn.Module):
    """æ‰©å±•çš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆåŒ…å«CNNã€LSTMã€Transformerä¸‰ä¸ªä¸“å®¶ï¼‰"""
    
    def __init__(self, vocab_size, embed_dim=64, num_classes=2, max_length=40):
        super(ExtendedMoEModel, self).__init__()
        
        self.num_experts = 3
        self.vocab_size = vocab_size
        
        # åˆ›å»ºä¸‰ä¸ªä¸“å®¶
        self.cnn_expert = CNNExpert(vocab_size, embed_dim, num_classes, max_length)
        self.lstm_expert = LSTMExpert(vocab_size, embed_dim, 128, 2, num_classes, 0.3)
        self.transformer_expert = TransformerExpert(vocab_size, 128, 8, 2, num_classes, max_length, 0.1)
        
        # é—¨æ§ç½‘ç»œï¼ˆè¾“å…¥ç‰¹å¾ç»´åº¦ï¼šæœ‰æ•ˆé•¿åº¦ + å”¯ä¸€å­—ç¬¦æ•° + å­—ç¬¦é¢‘ç‡ï¼‰
        self.gating_network = GatingNetwork(input_dim=42, num_experts=3)
        
        # è´Ÿè½½å‡è¡¡æŸå¤±çš„æƒé‡
        self.load_balance_weight = 0.01
        
    def forward(self, x, return_expert_outputs=False):
        batch_size = x.size(0)
        
        # è·å–é—¨æ§æƒé‡
        gate_weights = self.gating_network(x)  # (batch, 3)
        
        # è·å–å„ä¸“å®¶çš„è¾“å‡º
        cnn_output = self.cnn_expert(x)
        lstm_output = self.lstm_expert(x)
        transformer_output = self.transformer_expert(x)
        
        # ä¸“å®¶è¾“å‡ºå †å 
        expert_outputs = torch.stack([cnn_output, lstm_output, transformer_output], dim=2)  # (batch, num_classes, num_experts)
        
        # æ ¹æ®é—¨æ§æƒé‡åŠ æƒæ±‚å’Œ
        gate_weights = gate_weights.unsqueeze(1)  # (batch, 1, num_experts)
        final_output = torch.sum(expert_outputs * gate_weights, dim=2)  # (batch, num_classes)
        
        if return_expert_outputs:
            return final_output, gate_weights.squeeze(1), expert_outputs
        
        return final_output, gate_weights.squeeze(1)
    
    def load_balance_loss(self, gate_weights):
        """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±"""
        # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰æ‹©çš„å¹³å‡æ¦‚ç‡
        expert_usage = torch.mean(gate_weights, dim=0)  # (num_experts,)
        
        # ç†æƒ³æƒ…å†µä¸‹æ¯ä¸ªä¸“å®¶è¢«é€‰æ‹©çš„æ¦‚ç‡åº”è¯¥ç›¸ç­‰
        target_usage = 1.0 / self.num_experts
        
        # è®¡ç®—ä¸å¹³è¡¡ç¨‹åº¦
        load_loss = torch.sum((expert_usage - target_usage) ** 2)
        
        return load_loss
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'model_size_mb': total_params * 4 / 1024 / 1024,
            'num_experts': self.num_experts
        }


def load_dataset(file_path='./data/small_dga_dataset.pkl'):
    """åŠ è½½æ•°æ®é›†"""
    with open(file_path, 'rb') as f:
        dataset = pickle.load(f)
    return dataset


def main():
    """ä¸»å‡½æ•° - è®­ç»ƒæ‰©å±•MoEæ¨¡å‹"""
    print("=== DGAæ¶æ„åŸŸåæ£€æµ‹ - æ‰©å±•MoEæ¶æ„è®­ç»ƒï¼ˆCNN+LSTM+Transformerï¼‰ ===\n")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®é›†
    print("åŠ è½½æ•°æ®é›†...")
    dataset = load_dataset('./data/small_dga_dataset.pkl')
    X, y = dataset['X'], dataset['y']
    vocab_size = dataset['vocab_size']
    
    # åˆ›å»ºæ•°æ®é›†
    full_dataset = DGADataset(X, y)
    
    # æ•°æ®é›†åˆ’åˆ†
    total_size = len(full_dataset)
    train_size = int(0.7 * total_size)
    val_size = int(0.15 * total_size)
    test_size = total_size - train_size - val_size
    
    train_dataset, val_dataset, test_dataset = random_split(
        full_dataset, [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = 32
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # åˆ›å»ºæ‰©å±•MoEæ¨¡å‹
    model = ExtendedMoEModel(vocab_size=vocab_size, embed_dim=64, num_classes=2, max_length=40)
    model.to(device)
    
    print(f"æ‰©å±•MoEæ¨¡å‹å‚æ•°é‡: {model.get_model_info()['total_params']:,}")
    
    # è®­ç»ƒå‚æ•°
    num_epochs = 25
    learning_rate = 0.001
    
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)
    
    # è®­ç»ƒå¾ªç¯
    best_val_acc = 0
    patience = 10
    patience_counter = 0
    
    print("å¼€å§‹è®­ç»ƒæ‰©å±•MoEæ¨¡å‹...")
    
    for epoch in range(1, num_epochs + 1):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        epoch_gate_weights = []
        
        for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch} Training")):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output, gate_weights = model(data)
            
            # åˆ†ç±»æŸå¤± + è´Ÿè½½å‡è¡¡æŸå¤±
            classification_loss = criterion(output, target)
            load_balance_loss = model.load_balance_loss(gate_weights)
            total_loss = classification_loss + model.load_balance_weight * load_balance_loss
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += total_loss.item()
            pred = output.argmax(dim=1)
            train_correct += pred.eq(target).sum().item()
            train_total += target.size(0)
            
            epoch_gate_weights.append(gate_weights.detach().cpu())
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        val_gate_weights = []
        
        with torch.no_grad():
            for data, target in tqdm(val_loader, desc=f"Epoch {epoch} Validation"):
                data, target = data.to(device), target.to(device)
                output, gate_weights = model(data)
                
                classification_loss = criterion(output, target)
                load_balance_loss = model.load_balance_loss(gate_weights)
                total_loss = classification_loss + model.load_balance_weight * load_balance_loss
                
                val_loss += total_loss.item()
                pred = output.argmax(dim=1)
                val_correct += pred.eq(target).sum().item()
                val_total += target.size(0)
                
                val_gate_weights.append(gate_weights.cpu())
        
        # è®¡ç®—æŒ‡æ ‡
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        # ä¸“å®¶ä½¿ç”¨ç‡
        train_expert_usage = torch.mean(torch.cat(epoch_gate_weights, dim=0), dim=0)
        val_expert_usage = torch.mean(torch.cat(val_gate_weights, dim=0), dim=0)
        
        print(f"Epoch {epoch}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")
        print(f"Expert Usage - CNN: {train_expert_usage[0]:.3f}, LSTM: {train_expert_usage[1]:.3f}, Transformer: {train_expert_usage[2]:.3f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            torch.save(model.state_dict(), './data/best_extended_moe_model.pth')
            print(f"æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%ï¼Œæ¨¡å‹å·²ä¿å­˜")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"éªŒè¯å‡†ç¡®ç‡è¿ç»­{patience}ä¸ªepochæœªæå‡ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                break
        
        scheduler.step(val_acc)
    
    # æµ‹è¯•è¯„ä¼°
    model.load_state_dict(torch.load('./data/best_extended_moe_model.pth'))
    print(f"\nåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ‰©å±•MoEæ¨¡å‹...")
    
    model.eval()
    test_correct = 0
    test_total = 0
    test_gate_weights = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output, gate_weights = model(data)
            
            pred = output.argmax(dim=1)
            test_correct += pred.eq(target).sum().item()
            test_total += target.size(0)
            
            test_gate_weights.append(gate_weights.cpu())
    
    test_acc = 100. * test_correct / test_total
    test_expert_usage = torch.mean(torch.cat(test_gate_weights, dim=0), dim=0)
    
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
    print(f"æµ‹è¯•é›†ä¸“å®¶ä½¿ç”¨ç‡ - CNN: {test_expert_usage[0]:.3f}, LSTM: {test_expert_usage[1]:.3f}, Transformer: {test_expert_usage[2]:.3f}")
    
    print(f"\nğŸ‰ æ‰©å±•MoEè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DGAæ¶æ„åŸŸåæ£€æµ‹ - ç®€åŒ–ç‰ˆMambaæ¨¡å‹
åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„é«˜æ•ˆåºåˆ—å»ºæ¨¡ï¼ˆç®€åŒ–å®ç°ï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import pickle
import os
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from tqdm import tqdm
import time


class DGADataset(Dataset):
    """DGAæ•°æ®é›†ç±»"""
    
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.long)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


class SimplifiedMambaBlock(nn.Module):
    """ç®€åŒ–ç‰ˆMambaå— - ä½¿ç”¨RNNå’Œé—¨æ§æœºåˆ¶æ¨¡æ‹ŸçŠ¶æ€ç©ºé—´æ¨¡å‹"""
    
    def __init__(self, d_model, d_state=16, dropout=0.1):
        super(SimplifiedMambaBlock, self).__init__()
        
        self.d_model = d_model
        self.d_state = d_state
        
        # çŠ¶æ€ç©ºé—´å‚æ•°
        self.state_proj = nn.Linear(d_model, d_state)
        self.input_proj = nn.Linear(d_model, d_state)
        self.output_proj = nn.Linear(d_state, d_model)
        
        # é—¨æ§æœºåˆ¶
        self.gate = nn.Linear(d_model, d_model)
        
        # å½’ä¸€åŒ–
        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
        # çŠ¶æ€è½¬ç§»çŸ©é˜µï¼ˆå¯å­¦ä¹ ï¼‰
        self.state_transition = nn.Parameter(torch.randn(d_state, d_state) * 0.01)
        
    def forward(self, x):
        """
        x: (batch, seq_len, d_model)
        """
        batch_size, seq_len, _ = x.shape
        
        # æ®‹å·®è¿æ¥
        residual = x
        x = self.norm(x)
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h = torch.zeros(batch_size, self.d_state, device=x.device)
        
        outputs = []
        for t in range(seq_len):
            # å½“å‰è¾“å…¥
            x_t = x[:, t, :]  # (batch, d_model)
            
            # çŠ¶æ€æ›´æ–°
            h_proj = self.state_proj(x_t)  # (batch, d_state)
            x_proj = self.input_proj(x_t)  # (batch, d_state)
            
            # ç®€åŒ–çš„çŠ¶æ€ç©ºé—´æ›´æ–°
            h = torch.tanh(h @ self.state_transition.T + h_proj + x_proj)
            
            # è¾“å‡ºæŠ•å½±
            output_t = self.output_proj(h)  # (batch, d_model)
            
            # é—¨æ§
            gate_t = torch.sigmoid(self.gate(x_t))
            output_t = output_t * gate_t
            
            outputs.append(output_t)
        
        # æ‹¼æ¥æ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å‡º
        output = torch.stack(outputs, dim=1)  # (batch, seq_len, d_model)
        
        # æ®‹å·®è¿æ¥å’Œdropout
        output = self.dropout(output) + residual
        
        return output


class SimplifiedMambaModel(nn.Module):
    """ç®€åŒ–ç‰ˆMambaæ¨¡å‹"""
    
    def __init__(self, vocab_size, d_model=256, n_layers=4, d_state=16, 
                 num_classes=2, dropout=0.1):
        super(SimplifiedMambaModel, self).__init__()
        
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
        
        # Mambaå±‚
        self.layers = nn.ModuleList([
            SimplifiedMambaBlock(d_model, d_state, dropout)
            for _ in range(n_layers)
        ])
        
        self.norm = nn.LayerNorm(d_model)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, num_classes)
        )
        
        self.dropout = nn.Dropout(dropout)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, std=0.02)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, std=0.02)
    
    def forward(self, x):
        """
        x: (batch, seq_len)
        """
        # åµŒå…¥
        x = self.embedding(x) * math.sqrt(self.d_model)  # (batch, seq_len, d_model)
        x = self.dropout(x)
        
        # Mambaå±‚
        for layer in self.layers:
            x = layer(x)
        
        x = self.norm(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = torch.mean(x, dim=1)  # (batch, d_model)
        
        # åˆ†ç±»
        logits = self.classifier(x)
        
        return logits
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'model_size_mb': total_params * 4 / 1024 / 1024,
            'n_layers': len(self.layers),
            'd_model': self.d_model
        }


def load_dataset(file_path='./data/small_dga_dataset.pkl'):
    """åŠ è½½æ•°æ®é›†"""
    with open(file_path, 'rb') as f:
        dataset = pickle.load(f)
    return dataset


def train_mamba_model():
    """è®­ç»ƒç®€åŒ–ç‰ˆMambaæ¨¡å‹"""
    print("=== DGAæ¶æ„åŸŸåæ£€æµ‹ - ç®€åŒ–ç‰ˆMambaæ¨¡å‹è®­ç»ƒ ===\\n")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®é›†
    print("åŠ è½½æ•°æ®é›†...")
    if not os.path.exists('./data/small_dga_dataset.pkl'):
        print("æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ dataset_builder.py")
        return
    
    dataset = load_dataset('./data/small_dga_dataset.pkl')
    X, y = dataset['X'], dataset['y']
    vocab_size = dataset['vocab_size']
    max_length = dataset['max_length']
    
    print(f"æ•°æ®é›†ä¿¡æ¯:")
    print(f"  æ ·æœ¬æ•°: {len(X)}")
    print(f"  ç‰¹å¾ç»´åº¦: {X.shape}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
    print(f"  è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"  æœ€å¤§é•¿åº¦: {max_length}")
    
    # åˆ›å»ºæ•°æ®é›†
    full_dataset = DGADataset(X, y)
    
    # æ•°æ®é›†åˆ’åˆ† (70% è®­ç»ƒ, 15% éªŒè¯, 15% æµ‹è¯•)
    total_size = len(full_dataset)
    train_size = int(0.7 * total_size)
    val_size = int(0.15 * total_size)
    test_size = total_size - train_size - val_size
    
    train_dataset, val_dataset, test_dataset = random_split(
        full_dataset, [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    print(f"\\næ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(train_dataset)}")
    print(f"  éªŒè¯é›†: {len(val_dataset)}")
    print(f"  æµ‹è¯•é›†: {len(test_dataset)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = 32
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # åˆ›å»ºç®€åŒ–ç‰ˆMambaæ¨¡å‹
    print("\\nåˆ›å»ºç®€åŒ–ç‰ˆMambaæ¨¡å‹...")
    model = SimplifiedMambaModel(
        vocab_size=vocab_size, 
        d_model=128,  # å‡å°æ¨¡å‹å¤§å°ä»¥æé«˜è®­ç»ƒé€Ÿåº¦
        n_layers=3, 
        d_state=16,
        num_classes=2, 
        dropout=0.1
    )
    model.to(device)
    
    model_info = model.get_model_info()
    print(f"ç®€åŒ–ç‰ˆMambaæ¨¡å‹å‚æ•°é‡: {model_info['total_params']:,}")
    print(f"ç®€åŒ–ç‰ˆMambaæ¨¡å‹å¤§å°: {model_info['model_size_mb']:.2f} MB")
    
    # è®­ç»ƒå‚æ•°
    num_epochs = 15
    learning_rate = 0.001
    
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=3)
    
    # è®­ç»ƒå¾ªç¯
    best_val_acc = 0
    patience = 6
    patience_counter = 0
    
    print("å¼€å§‹è®­ç»ƒç®€åŒ–ç‰ˆMambaæ¨¡å‹...")
    start_time = time.time()
    
    for epoch in range(1, num_epochs + 1):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch} Training")):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            pred = output.argmax(dim=1)
            train_correct += pred.eq(target).sum().item()
            train_total += target.size(0)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for data, target in tqdm(val_loader, desc=f"Epoch {epoch} Validation"):
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                pred = output.argmax(dim=1)
                val_correct += pred.eq(target).sum().item()
                val_total += target.size(0)
        
        # è®¡ç®—æŒ‡æ ‡
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        print(f"Epoch {epoch}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")
        print(f"Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}")
        print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            torch.save(model.state_dict(), './data/best_simplified_mamba_model.pth')
            print(f"æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%ï¼Œæ¨¡å‹å·²ä¿å­˜")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"éªŒè¯å‡†ç¡®ç‡è¿ç»­{patience}ä¸ªepochæœªæå‡ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                break
        
        scheduler.step(val_acc)
    
    training_time = time.time() - start_time
    print(f"\\nç®€åŒ–ç‰ˆMambaè®­ç»ƒå®Œæˆï¼ç”¨æ—¶: {training_time:.2f}ç§’")
    
    # æµ‹è¯•è¯„ä¼°
    model.load_state_dict(torch.load('./data/best_simplified_mamba_model.pth'))
    print(f"\\nåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³ç®€åŒ–ç‰ˆMambaæ¨¡å‹...")
    
    model.eval()
    all_preds = []
    all_targets = []
    inference_times = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            
            # æµ‹é‡æ¨ç†æ—¶é—´
            start_time_inference = time.time()
            output = model(data)
            inference_time = time.time() - start_time_inference
            inference_times.append(inference_time)
            
            pred = output.argmax(dim=1)
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_targets, all_preds)
    precision = precision_score(all_targets, all_preds, average='weighted')
    recall = recall_score(all_targets, all_preds, average='weighted')
    f1 = f1_score(all_targets, all_preds, average='weighted')
    
    print(f"\\n=== ç®€åŒ–ç‰ˆMambaè¯¦ç»†è¯„ä¼°ç»“æœ ===")
    print(f"å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"ç²¾ç¡®ç‡: {precision:.4f}")
    print(f"å¬å›ç‡: {recall:.4f}")
    print(f"F1åˆ†æ•°: {f1:.4f}")
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {np.mean(inference_times)*1000:.2f}ms")
    
    # åˆ†ç±»æŠ¥å‘Š
    print(f"\\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_targets, all_preds, target_names=['Benign', 'Malicious']))
    
    # ä¿å­˜ç»“æœ
    results = {
        'model_name': 'Simplified Mamba',
        'model_info': model_info,
        'best_val_accuracy': best_val_acc,
        'test_results': {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'inference_time_ms': np.mean(inference_times) * 1000
        }
    }
    
    with open('./data/simplified_mamba_results.pkl', 'wb') as f:
        pickle.dump(results, f)
    
    print(f"\\nğŸ‰ ç®€åŒ–ç‰ˆMambaè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"ç»“æœå·²ä¿å­˜åˆ° ./data/simplified_mamba_results.pkl")
    
    return accuracy, model_info


if __name__ == "__main__":
    import math
    train_mamba_model()
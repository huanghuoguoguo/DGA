#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DGA Detection - MambaFormeræ¨¡å‹æµ‹è¯•è„šæœ¬
"""

import torch
import pickle
import sys
import os
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.append(project_root)

from core.dataset import create_data_loaders, print_dataset_info
from core.base_model import ModelTrainer
from config.config import config
from models.implementations.mambaformer_model import MambaFormerModel


def test_mambaformer_architectures():
    """æµ‹è¯•ä¸åŒçš„MambaFormerèåˆç­–ç•¥"""
    print("ğŸš€ MambaFormeræ¶æ„æµ‹è¯•")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡å’Œéšæœºç§å­
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    torch.manual_seed(42)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®é›†
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    try:
        train_loader, val_loader, test_loader, dataset_info = create_data_loaders(
            dataset_path=config.data.dataset_path,
            batch_size=16,  # å‡å°batch sizeä»¥é€‚åº”æ›´å¤§çš„æ¨¡å‹
            random_seed=42
        )
        print_dataset_info(dataset_info)
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        return
    
    # æµ‹è¯•ä¸åŒçš„èåˆç­–ç•¥
    fusion_types = ['sequential', 'parallel', 'gated']
    results = {}
    
    for fusion_type in fusion_types:
        print(f"\n{'='*60}")
        print(f"ğŸ§ª æµ‹è¯• {fusion_type.upper()} MambaFormer")
        print(f"{'='*60}")
        
        # åˆ›å»ºæ¨¡å‹
        model = MambaFormerModel(
            vocab_size=dataset_info['vocab_size'],
            d_model=128,  # å‡å°æ¨¡å‹ä»¥åŠ å¿«è®­ç»ƒ
            n_layers=2,   # å‡å°‘å±‚æ•°
            d_state=16,
            n_heads=4,    # å‡å°‘æ³¨æ„åŠ›å¤´æ•°
            num_classes=2,
            dropout=0.1,
            fusion_type=fusion_type
        )
        
        print(f"ğŸ“‹ {fusion_type.upper()} MambaFormeræ¨¡å‹ä¿¡æ¯:")
        model.print_model_info()
        fusion_info = model.get_fusion_info()
        print(f"  èåˆç­–ç•¥: {fusion_info['fusion_type']}")
        print(f"  æ¶æ„ç±»å‹: {fusion_info['architecture']}")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ModelTrainer(model, device)
        
        # å¿«é€Ÿè®­ç»ƒï¼ˆå°‘æ•°epochsç”¨äºæµ‹è¯•ï¼‰
        save_path = f"./data/models/best_mambaformer_{fusion_type}_model.pth"
        
        start_time = time.time()
        training_results = trainer.train(
            train_loader=train_loader,
            val_loader=val_loader,
            num_epochs=5,  # å¿«é€Ÿæµ‹è¯•
            learning_rate=0.001,
            weight_decay=1e-4,
            patience=3,
            save_path=save_path
        )
        training_time = time.time() - start_time
        
        # æµ‹è¯•è¯„ä¼°
        if os.path.exists(save_path):
            model.load_state_dict(torch.load(save_path))
            test_results = trainer.evaluate(test_loader)
            
            # ä¿å­˜ç»“æœ
            result = {
                'fusion_type': fusion_type,
                'model_info': model.get_model_info(),
                'fusion_info': model.get_fusion_info(),
                'training_results': training_results,
                'test_results': test_results,
                'training_time': training_time
            }
            
            results[fusion_type] = result
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            result_path = f"./data/results/mambaformer_{fusion_type}_results.pkl"
            with open(result_path, 'wb') as f:
                pickle.dump(result, f)
            
            print(f"âœ… {fusion_type.upper()} MambaFormeræµ‹è¯•å®Œæˆ:")
            print(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {training_results['best_val_accuracy']:.2f}%")
            print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_results['accuracy']*100:.2f}%")
            print(f"  F1åˆ†æ•°: {test_results['f1']:.4f}")
            print(f"  æ¨ç†æ—¶é—´: {test_results['inference_time_ms']:.2f}ms")
            print(f"  è®­ç»ƒç”¨æ—¶: {training_time:.1f}ç§’")
            print(f"  ç»“æœå·²ä¿å­˜: {result_path}")
    
    # å¯¹æ¯”åˆ†æ
    if results:
        print(f"\n{'='*80}")
        print("ğŸ“Š MambaFormeræ¶æ„å¯¹æ¯”åˆ†æ")
        print(f"{'='*80}")
        
        print(f"{'èåˆç­–ç•¥':<12} {'éªŒè¯å‡†ç¡®ç‡':<12} {'æµ‹è¯•å‡†ç¡®ç‡':<12} {'F1åˆ†æ•°':<10} {'æ¨ç†æ—¶é—´(ms)':<12} {'å‚æ•°é‡':<12}")
        print("-" * 80)
        
        best_fusion = None
        best_accuracy = 0
        
        for fusion_type, result in results.items():
            val_acc = result['training_results']['best_val_accuracy']
            test_acc = result['test_results']['accuracy'] * 100
            f1_score = result['test_results']['f1']
            inference_time = result['test_results']['inference_time_ms']
            params = result['model_info']['total_params']
            
            print(f"{fusion_type.upper():<12} {val_acc:<11.2f}% {test_acc:<11.2f}% "
                  f"{f1_score:<9.4f} {inference_time:<11.2f} {params:<11,}")
            
            if test_acc > best_accuracy:
                best_accuracy = test_acc
                best_fusion = fusion_type
        
        print(f"\nğŸ† æœ€ä½³èåˆç­–ç•¥: {best_fusion.upper()} ({best_accuracy:.2f}%)")
        
        # èåˆç­–ç•¥åˆ†æ
        print(f"\nğŸ’¡ èåˆç­–ç•¥åˆ†æ:")
        print(f"  Sequential: Mambaå¤„ç†åå†ç”¨Transformerï¼Œé€‚åˆé¡ºåºç‰¹å¾æå–")
        print(f"  Parallel: å¹¶è¡Œå¤„ç†åèåˆï¼Œå¹³è¡¡ä¸¤ç§æ¶æ„çš„ä¼˜åŠ¿")
        print(f"  Gated: åŠ¨æ€æƒé‡èåˆï¼Œæ™ºèƒ½é€‰æ‹©æœ€é€‚åˆçš„ç‰¹å¾")
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        comparison_result = {
            'comparison_type': 'MambaFormer_Fusion_Strategies',
            'results': results,
            'best_fusion': best_fusion,
            'best_accuracy': best_accuracy
        }
        
        with open('./data/results/mambaformer_comparison.pkl', 'wb') as f:
            pickle.dump(comparison_result, f)
        
        print(f"\nğŸ’¾ å¯¹æ¯”ç»“æœå·²ä¿å­˜: ./data/results/mambaformer_comparison.pkl")
    
    return results


def test_forward_pass():
    """æµ‹è¯•MambaFormerå‰å‘ä¼ æ’­"""
    print("\nğŸ§ª MambaFormerå‰å‘ä¼ æ’­æµ‹è¯•")
    print("-" * 40)
    
    vocab_size = 40
    seq_length = 20
    batch_size = 2
    
    # æµ‹è¯•ä¸åŒèåˆç­–ç•¥
    for fusion_type in ['sequential', 'parallel', 'gated']:
        try:
            model = MambaFormerModel(
                vocab_size=vocab_size,
                d_model=64,
                n_layers=2,
                fusion_type=fusion_type
            )
            
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            x = torch.randint(0, vocab_size, (batch_size, seq_length))
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                output = model(x)
            
            print(f"âœ… {fusion_type.upper()}: è¾“å…¥{x.shape} â†’ è¾“å‡º{output.shape}")
            
        except Exception as e:
            print(f"âŒ {fusion_type.upper()}: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ DGAæ£€æµ‹ - MambaFormeræ¨¡å‹æµ‹è¯•")
    print("=" * 80)
    
    # åŸºç¡€å‰å‘ä¼ æ’­æµ‹è¯•
    test_forward_pass()
    
    # å®Œæ•´æ¶æ„æµ‹è¯•
    results = test_mambaformer_architectures()
    
    if results:
        print(f"\nğŸ‰ MambaFormeræµ‹è¯•å®Œæˆï¼")
        print(f"å…±æµ‹è¯•äº† {len(results)} ç§èåˆç­–ç•¥")
        print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° ./data/results/ ç›®å½•")
    else:
        print(f"\nâŒ æµ‹è¯•æœªå®Œæˆï¼Œè¯·æ£€æŸ¥æ•°æ®é›†å’Œç¯å¢ƒé…ç½®")


if __name__ == "__main__":
    main()